{
  "version": 4,
  "terraform_version": "0.14.7",
  "serial": 9,
  "lineage": "c2220fdf-c84d-69f9-7d17-737ad8aaa171",
  "outputs": {
    "controller_network_ips": {
      "value": [
        "10.0.0.2"
      ],
      "type": [
        "tuple",
        [
          "string"
        ]
      ]
    },
    "login_network_ips": {
      "value": [
        "10.0.0.3"
      ],
      "type": [
        "tuple",
        [
          "string"
        ]
      ]
    }
  },
  "resources": [
    {
      "module": "module.slurm_cluster_controller",
      "mode": "data",
      "type": "google_compute_default_service_account",
      "name": "default",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "display_name": "Compute Engine default service account",
            "email": "909139830476-compute@developer.gserviceaccount.com",
            "id": "projects/hpc-apps/serviceAccounts/909139830476-compute@developer.gserviceaccount.com",
            "name": "projects/hpc-apps/serviceAccounts/909139830476-compute@developer.gserviceaccount.com",
            "project": "hpc-apps",
            "unique_id": "113961341457955783623"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.slurm_cluster_controller",
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "controller_node",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": null,
            "attached_disk": [],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/images/wrf-gcp-v1",
                    "labels": {},
                    "size": 250,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/hpc-apps/zones/us-west1-b/disks/wrf-demo-controller"
              }
            ],
            "can_ip_forward": false,
            "confidential_instance_config": [],
            "cpu_platform": "Intel Broadwell",
            "current_status": "RUNNING",
            "deletion_protection": false,
            "description": "",
            "desired_status": null,
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/hpc-apps/zones/us-west1-b/instances/wrf-demo-controller",
            "instance_id": "3879451585784328975",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-16",
            "metadata": {
              "VmDnsSetting": "GlobalOnly",
              "cgroup_conf_tpl": "CgroupAutomount=no\n#CgroupMountpoint=/sys/fs/cgroup\nConstrainCores=yes\nConstrainRamSpace=yes\nConstrainSwapSpace=yes\nTaskAffinity=no\nConstrainDevices=yes\n",
              "config": "{\"cloudsql\":null,\"cluster_name\":\"wrf-demo\",\"compute_node_scopes\":[\"https://www.googleapis.com/auth/cloud-platform\"],\"compute_node_service_account\":\"default\",\"controller_secondary_disk\":false,\"external_compute_ips\":false,\"jwt_key\":null,\"login_network_storage\":[],\"login_node_count\":1,\"munge_key\":null,\"network_storage\":[],\"partitions\":[{\"compute_disk_size_gb\":50,\"compute_disk_type\":\"pd-standard\",\"compute_labels\":{},\"cpu_platform\":null,\"enable_placement\":false,\"exclusive\":false,\"gpu_count\":0,\"gpu_type\":null,\"image\":\"projects/hpc-apps/global/images/wrf-gcp-v1\",\"image_hyperthreads\":true,\"instance_template\":null,\"machine_type\":\"c2-standard-8\",\"max_node_count\":3,\"name\":\"wrf\",\"network_storage\":[],\"preemptible_bursting\":false,\"regional_capacity\":false,\"regional_policy\":null,\"static_node_count\":0,\"vpc_subnet\":null,\"zone\":\"us-west1-b\"}],\"project\":\"hpc-apps\",\"region\":\"us-west1\",\"shared_vpc_host_project\":null,\"suspend_time\":300,\"vpc_subnet\":null,\"zone\":\"us-west1-b\"}",
              "custom-compute-install": "",
              "custom-controller-install": "",
              "enable-oslogin": "TRUE",
              "setup-script": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport logging\nimport os\nimport sys\nimport shutil\nimport time\nfrom pathlib import Path\nfrom subprocess import DEVNULL\nfrom functools import reduce, partialmethod\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport googleapiclient.discovery\nimport requests\nimport yaml\n\n\n# get util.py from metadata\nUTIL_FILE = Path('/tmp/util.py')\ntry:\n    resp = requests.get('http://metadata.google.internal/computeMetadata/v1/instance/attributes/util-script',\n                        headers={'Metadata-Flavor': 'Google'})\n    resp.raise_for_status()\n    UTIL_FILE.write_text(resp.text)\nexcept requests.exceptions.RequestException:\n    print(\"util.py script not found in metadata\")\n    if not UTIL_FILE.exists():\n        print(f\"{UTIL_FILE} also does not exist, aborting\")\n        sys.exit(1)\n\nspec = importlib.util.spec_from_file_location('util', UTIL_FILE)\nutil = importlib.util.module_from_spec(spec)\nsys.modules[spec.name] = util\nspec.loader.exec_module(util)\ncd = util.cd  # import util.cd into local namespace\nNSDict = util.NSDict\n\nPath.mkdirp = partialmethod(Path.mkdir, parents=True, exist_ok=True)\n\nutil.config_root_logger(logfile='/tmp/setup.log')\nlog = logging.getLogger(Path(__file__).name)\nsys.excepthook = util.handle_exception\n\n# get setup config from metadata\nconfig_yaml = yaml.safe_load(util.get_metadata('attributes/config'))\ncfg = util.Config.new_config(config_yaml)\n\n# load all directories as Paths into a dict-like namespace\ndirs = NSDict({n: Path(p) for n, p in dict.items({\n    'home': '/home',\n    'apps': '/apps',\n    'scripts': '/slurm/scripts',\n    'slurm': '/slurm',\n    'prefix': '/usr/local',\n    'munge': '/etc/munge',\n    'secdisk': '/mnt/disks/sec',\n})})\n\nslurmdirs = NSDict({n: Path(p) for n, p in dict.items({\n    'etc': '/usr/local/etc/slurm',\n    'log': '/var/log/slurm',\n    'state': '/var/spool/slurm',\n})})\n\ncfg['log_dir'] = slurmdirs.log\ncfg['slurm_cmd_path'] = dirs.prefix/'bin'\n\nRESUME_TIMEOUT = 300\nSUSPEND_TIMEOUT = 300\n\nCONTROL_MACHINE = cfg.cluster_name + '-controller'\n\nMOTD_HEADER = \"\"\"\n\n                                 SSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                        SSSS     SSSSSSS     SSSS\n                       SSSSSS               SSSSSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                        SSSS    SSSSSSSSS    SSSS\n                SSS             SSSSSSSSS             SSS\n               SSSSS    SSSS    SSSSSSSSS    SSSS    SSSSS\n                SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                SSS    SSSSSS               SSSSSS    SSS\n               SSSSS    SSSS     SSSSSSS     SSSS    SSSSS\n          S     SSS             SSSSSSSSS             SSS     S\n         SSS            SSSS    SSSSSSSSS    SSSS            SSS\n          S     SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS     S\n               SSSSS   SSSSSS   SSSSSSSSS   SSSSSS   SSSSS\n          S    SSSSS    SSSS     SSSSSSS     SSSS    SSSSS    S\n    S    SSS    SSS                                   SSS    SSS    S\n    S     S                                                   S     S\n                SSS\n                SSS\n                SSS\n                SSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS    SSSSSSSSS   SSSSSSSSSSSSSSSSSSSS\nSSSSSSSSSSSSS   SSS   SSSS       SSSS   SSSSSSSSSS  SSSSSSSSSSSSSSSSSSSSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSSS   SSS   SSSSSSSSSSSSSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS    SSSSSSSSSSSSS    SSSS        SSSS     SSSS     SSSS\n\n\n\"\"\"\n\n\ndef start_motd():\n    \"\"\" advise in motd that slurm is currently configuring \"\"\"\n    msg = MOTD_HEADER + \"\"\"\n*** Slurm is currently being configured in the background. ***\n\"\"\"\n    Path('/etc/motd').write_text(msg)\n# END start_motd()\n\n\ndef end_motd(broadcast=True):\n    \"\"\" modify motd to signal that setup is complete \"\"\"\n    Path('/etc/motd').write_text(MOTD_HEADER)\n\n    if not broadcast:\n        return\n\n    util.run(\"wall -n '*** Slurm {} setup complete ***'\"\n             .format(cfg.instance_type))\n    if cfg.instance_type != 'controller':\n        util.run(\"\"\"wall -n '\n/home on the controller was mounted over the existing /home.\nLog back in to ensure your home directory is correct.\n'\"\"\")\n# END start_motd()\n\n\ndef expand_instance_templates():\n    \"\"\" Expand instance template into instance_defs \"\"\"\n\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for pid, instance_def in cfg.instance_defs.items():\n        if (instance_def.instance_template and\n                (not instance_def.machine_type or not instance_def.gpu_count)):\n            template_resp = util.ensure_execute(\n                compute.instanceTemplates().get(\n                    project=cfg.project,\n                    instanceTemplate=instance_def.instance_template))\n            if template_resp:\n                template_props = template_resp['properties']\n                if not instance_def.machine_type:\n                    instance_def.machine_type = template_props['machineType']\n                if (not instance_def.gpu_count and\n                        'guestAccelerators' in template_props):\n                    accel_props = template_props['guestAccelerators'][0]\n                    instance_def.gpu_count = accel_props['acceleratorCount']\n                    instance_def.gpu_type = accel_props['acceleratorType']\n# END expand_instance_templates()\n\n\ndef expand_machine_type():\n    \"\"\" get machine type specs from api \"\"\"\n    machines = {}\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for pid, part in cfg.instance_defs.items():\n        machine = {'cpus': 1, 'memory': 1}\n        machines[pid] = machine\n\n        if not part.machine_type:\n            log.error(\"No machine type to get configuration from\")\n            continue\n\n        type_resp = None\n        if part.regional_capacity:\n            filter = f\"(zone={part.region}-*) AND (name={part.machine_type})\"\n            list_resp = util.ensure_execute(\n                compute.machineTypes().aggregatedList(\n                    project=cfg.project, filter=filter))\n\n            if 'items' in list_resp:\n                zone_types = list_resp['items']\n                for k, v in zone_types.items():\n                    if part.region in k and 'machineTypes' in v:\n                        type_resp = v['machineTypes'][0]\n                        break\n        else:\n            type_resp = util.ensure_execute(\n                compute.machineTypes().get(\n                    project=cfg.project, zone=part.zone,\n                    machineType=part.machine_type))\n\n        if type_resp:\n            cpus = type_resp['guestCpus']\n            machine['cpus'] = cpus // (1 if part.image_hyperthreads else 2)\n\n            # Because the actual memory on the host will be different than\n            # what is configured (e.g. kernel will take it). From\n            # experiments, about 16 MB per GB are used (plus about 400 MB\n            # buffer for the first couple of GB's. Using 30 MB to be safe.\n            gb = type_resp['memoryMb'] // 1024\n            machine['memory'] = type_resp['memoryMb'] - (400 + (gb * 30))\n\n    return machines\n# END expand_machine_type()\n\n\ndef install_slurm_conf():\n    \"\"\" install slurm.conf \"\"\"\n    machines = expand_machine_type()\n\n    if cfg.ompi_version:\n        mpi_default = \"pmi2\"\n    else:\n        mpi_default = \"none\"\n\n    conf_options = {\n        'name': cfg.cluster_name,\n        'control_host': CONTROL_MACHINE,\n        'scripts': dirs.scripts,\n        'slurmlog': slurmdirs.log,\n        'state_save': slurmdirs.state,\n        'resume_timeout': RESUME_TIMEOUT,\n        'suspend_timeout': SUSPEND_TIMEOUT,\n        'suspend_time': cfg.suspend_time,\n        'mpi_default': mpi_default,\n    }\n    conf_resp = util.get_metadata('attributes/slurm_conf_tpl')\n    conf = conf_resp.format(**conf_options)\n\n    static_nodes = []\n    for i, (pid, machine) in enumerate(machines.items()):\n        part = cfg.instance_defs[pid]\n        static_range = ''\n        if part.static_node_count:\n            if part.static_node_count \u003e 1:\n                static_range = '{}-[0-{}]'.format(\n                    pid, part.static_node_count - 1)\n            else:\n                static_range = f\"{pid}-0\"\n\n        cloud_range = \"\"\n        if (part.max_node_count and\n                (part.max_node_count != part.static_node_count)):\n            cloud_range = \"{}-[{}-{}]\".format(\n                pid, part.static_node_count,\n                part.max_node_count - 1)\n\n        conf += (\"NodeName=DEFAULT \"\n                 f\"CPUs={machine['cpus']} \"\n                 f\"RealMemory={machine['memory']} \"\n                 \"State=UNKNOWN\")\n        conf += '\\n'\n\n        # Nodes\n        gres = \"\"\n        if part.gpu_count:\n            gres = \" Gres=gpu:\" + str(part.gpu_count)\n        if static_range:\n            static_nodes.append(static_range)\n            conf += f\"NodeName={static_range}{gres}\\n\"\n\n        if cloud_range:\n            conf += f\"NodeName={cloud_range} State=CLOUD{gres}\\n\"\n\n        # instance_defs\n        part_nodes = f'{pid}-[0-{part.max_node_count - 1}]'\n\n        def_mem_per_cpu = max(100, machine['memory'] // machine['cpus'])\n\n        conf += (\"PartitionName={} Nodes={} MaxTime=INFINITE \"\n                 \"State=UP DefMemPerCPU={} LLN=yes\"\n                 .format(part.name, part_nodes,\n                         def_mem_per_cpu))\n        if part.exclusive:\n            conf += \" Oversubscribe=Exclusive\"\n\n        # First partition specified is treated as the default partition\n        if i == 0:\n            conf += \" Default=YES\"\n        conf += \"\\n\\n\"\n\n    if len(static_nodes):\n        conf += \"\\nSuspendExcNodes={}\\n\".format(','.join(static_nodes))\n\n    conf_file = slurmdirs.etc/'slurm.conf'\n    conf_file.write_text(conf)\n    shutil.chown(conf_file, user='slurm', group='slurm')\n# END install_slurm_conf()\n\n\ndef install_slurmdbd_conf():\n    \"\"\" install slurmdbd.conf \"\"\"\n    conf_options = NSDict({\n        'control_host': CONTROL_MACHINE,\n        'slurmlog': slurmdirs.log,\n        'state_save': slurmdirs.state,\n        'db_name': 'slurm_acct_db',\n        'db_user': 'slurm',\n        'db_pass': '\"\"',\n        'db_host': 'localhost',\n        'db_port': '3306'\n    })\n    if cfg.cloudsql:\n        conf_options.db_name = cfg.cloudsql.db_name\n        conf_options.db_user = cfg.cloudsql.user\n        conf_options.db_pass = cfg.cloudsql.password\n\n        db_host_str = cfg.cloudsql.server_ip.split(':')\n        conf_options.db_host = db_host_str[0]\n        conf_options.db_port = db_host_str[1] if len(db_host_str) \u003e= 2 else '3306'\n\n    conf_resp = util.get_metadata('attributes/slurmdbd_conf_tpl')\n    conf = conf_resp.format(**conf_options)\n\n    conf_file = slurmdirs.etc/'slurmdbd.conf'\n    conf_file.write_text(conf)\n    shutil.chown(conf_file, user='slurm', group='slurm')\n    conf_file.chmod(0o600)\n# END install_slurmdbd_conf()\n\n\ndef install_cgroup_conf():\n    \"\"\" install cgroup.conf \"\"\"\n    conf = util.get_metadata('attributes/cgroup_conf_tpl')\n\n    conf_file = slurmdirs.etc/'cgroup.conf'\n    conf_file.write_text(conf)\n    shutil.chown(conf_file, user='slurm', group='slurm')\n\n    gpu_conf = \"\"\n    for pid, part in cfg.instance_defs.items():\n        if not part.gpu_count:\n            continue\n        driver_range = '0'\n        if part.gpu_count \u003e 1:\n            driver_range = '[0-{}]'.format(part.gpu_count-1)\n\n        gpu_conf += (\"NodeName={}-[0-{}] Name=gpu File=/dev/nvidia{}\\n\"\n                     .format(pid, part.max_node_count - 1, driver_range))\n    if gpu_conf:\n        (slurmdirs.etc/'gres.conf').write_text(gpu_conf)\n# END install_cgroup_conf()\n\n\ndef install_meta_files():\n    \"\"\" save config.yaml and download all scripts from metadata \"\"\"\n    cfg.save_config(dirs.scripts/'config.yaml')\n    shutil.chown(dirs.scripts/'config.yaml', user='slurm', group='slurm')\n\n    meta_entries = [\n        ('suspend.py', 'slurm-suspend'),\n        ('resume.py', 'slurm-resume'),\n        ('slurmsync.py', 'slurmsync'),\n        ('util.py', 'util-script'),\n        ('setup.py', 'setup-script'),\n        ('startup.sh', 'startup-script'),\n        ('custom-compute-install', 'custom-compute-install'),\n        ('custom-controller-install', 'custom-controller-install'),\n    ]\n\n    def install_metafile(filename, metaname):\n        text = util.get_metadata('attributes/' + metaname)\n        if not text:\n            return\n        path = dirs.scripts/filename\n        path.write_text(text)\n        path.chmod(0o755)\n        shutil.chown(path, user='slurm', group='slurm')\n\n    with ThreadPoolExecutor() as exe:\n        exe.map(lambda x: install_metafile(*x), meta_entries)\n\n# END install_meta_files()\n\n\ndef prepare_network_mounts(hostname, instance_type):\n    \"\"\" Prepare separate lists of cluster-internal and external mounts for the\n    given host instance, returning (external_mounts, internal_mounts)\n    \"\"\"\n    log.info(\"Set up network storage\")\n\n    default_mounts = (\n        slurmdirs.etc,\n        dirs.munge,\n        dirs.home,\n        dirs.apps,\n    )\n\n    # create dict of mounts, local_mount: mount_info\n    CONTROL_NFS = {\n        'server_ip': CONTROL_MACHINE,\n        'remote_mount': 'none',\n        'local_mount': 'none',\n        'fs_type': 'nfs',\n        'mount_options': 'defaults,hard,intr',\n    }\n    # seed the non-controller mounts with the default controller mounts\n    mounts = {\n        path: util.Config(CONTROL_NFS, local_mount=path, remote_mount=path)\n        for path in default_mounts\n    }\n\n    # convert network_storage list of mounts to dict of mounts,\n    #   local_mount as key\n    def listtodict(mountlist):\n        return {Path(d['local_mount']).resolve(): d for d in mountlist}\n\n    # On non-controller instances, entries in network_storage could overwrite\n    # default exports from the controller. Be careful, of course\n    mounts.update(listtodict(cfg.network_storage))\n\n    if instance_type == 'compute':\n        pid = util.get_pid(hostname)\n        mounts.update(listtodict(cfg.instance_defs[pid].network_storage))\n    else:\n        # login_network_storage is mounted on controller and login instances\n        mounts.update(listtodict(cfg.login_network_storage))\n\n    # filter mounts into two dicts, cluster-internal and external mounts, and\n    # return both. (external_mounts, internal_mounts)\n    def internal_mount(mount):\n        return mount[1].server_ip == CONTROL_MACHINE\n\n    def partition(pred, coll):\n        \"\"\" filter into 2 lists based on pred returning True or False \n            returns ([False], [True])\n        \"\"\"\n        return reduce(\n            lambda acc, el: acc[pred(el)].append(el) or acc,\n            coll, ([], [])\n        )\n\n    return tuple(map(dict, partition(internal_mount, mounts.items())))\n# END prepare_network_mounts\n\n\ndef setup_network_storage():\n    \"\"\" prepare network fs mounts and add them to fstab \"\"\"\n\n    global mounts\n    ext_mounts, int_mounts = prepare_network_mounts(cfg.hostname,\n                                                    cfg.instance_type)\n    mounts = ext_mounts\n    if cfg.instance_type != 'controller':\n        mounts.update(int_mounts)\n\n    # Determine fstab entries and write them out\n    fstab_entries = []\n    for local_mount, mount in mounts.items():\n        remote_mount = mount.remote_mount\n        fs_type = mount.fs_type\n        server_ip = mount.server_ip\n\n        # do not mount controller mounts to itself\n        if server_ip == CONTROL_MACHINE and cfg.instance_type == 'controller':\n            continue\n\n        log.info(\"Setting up mount ({}) {}{} to {}\".format(\n            fs_type, server_ip+':' if fs_type != 'gcsfuse' else \"\",\n            remote_mount, local_mount))\n\n        local_mount.mkdirp()\n\n        mount_options = (mount.mount_options.split(',') if mount.mount_options\n                         else [])\n        if not mount_options or '_netdev' not in mount_options:\n            mount_options += ['_netdev']\n\n        if fs_type == 'gcsfuse':\n            if 'nonempty' not in mount_options:\n                mount_options += ['nonempty']\n            fstab_entries.append(\n                \"{0}   {1}     {2}     {3}     0 0\"\n                .format(remote_mount, local_mount, fs_type,\n                        ','.join(mount_options)))\n        else:\n            remote_mount = Path(remote_mount).resolve()\n            fstab_entries.append(\n                \"{0}:{1}    {2}     {3}      {4}  0 0\"\n                .format(server_ip, remote_mount, local_mount,\n                        fs_type, ','.join(mount_options)))\n\n    for mount in mounts:\n        Path(mount).mkdirp()\n    with open('/etc/fstab', 'a') as f:\n        f.write('\\n')\n        for entry in fstab_entries:\n            f.write(entry)\n            f.write('\\n')\n# END setup_network_storage()\n\n\ndef mount_fstab():\n    \"\"\" Wait on each mount, then make sure all fstab is mounted \"\"\"\n    global mounts\n\n    def mount_path(path):\n        while not os.path.ismount(path):\n            log.info(f\"Waiting for {path} to be mounted\")\n            util.run(f\"mount {path}\", wait=5)\n\n    with ThreadPoolExecutor() as exe:\n        exe.map(mount_path, mounts.keys())\n\n    util.run(\"mount -a\", wait=1)\n# END mount_external\n\n\ndef setup_nfs_exports():\n    \"\"\" nfs export all needed directories \"\"\"\n    # The controller only needs to set up exports for cluster-internal mounts\n    # switch the key to remote mount path since that is what needs exporting\n    _, con_mounts = prepare_network_mounts(cfg.hostname, cfg.instance_type)\n    con_mounts = {m.remote_mount: m for m in con_mounts.values()}\n    for pid, _ in cfg.instance_defs.items():\n        # get internal mounts for each partition by calling\n        # prepare_network_mounts as from a node in each partition\n        _, part_mounts = prepare_network_mounts(f'{pid}-n', 'compute')\n        part_mounts = {m.remote_mount: m for m in part_mounts.values()}\n        con_mounts.update(part_mounts)\n\n    # export path if corresponding selector boolean is True\n    exports = []\n    for path in con_mounts:\n        Path(path).mkdirp()\n        util.run(rf\"sed -i '\\#{path}#d' /etc/exports\")\n        exports.append(f\"{path}  *(rw,no_subtree_check,no_root_squash)\")\n\n    exportsd = Path('/etc/exports.d')\n    exportsd.mkdirp()\n    with (exportsd/'slurm.exports').open('w') as f:\n        f.write('\\n')\n        f.write('\\n'.join(exports))\n    util.run(\"exportfs -a\")\n# END setup_nfs_exports()\n\n\ndef setup_secondary_disks():\n    \"\"\" Format and mount secondary disk \"\"\"\n    util.run(\n        \"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\")\n    with open('/etc/fstab', 'a') as f:\n        f.write(\n            \"\\n/dev/sdb     {0}     ext4    discard,defaults,nofail     0 2\"\n            .format(dirs.secdisk))\n\n# END setup_secondary_disks()\n\n\ndef setup_sync_cronjob():\n    \"\"\" Create cronjob for running slurmsync.py \"\"\"\n    util.run(\"crontab -u slurm -\", input=(\n        f\"*/1 * * * * {dirs.scripts}/slurmsync.py\\n\"))\n\n# END setup_sync_cronjob()\n\n\ndef setup_jwt_key():\n    jwt_key = slurmdirs.state/'jwt_hs256.key'\n\n    if cfg.jwt_key:\n        with (jwt_key).open('w') as f:\n            f.write(cfg.jwt_key)\n    else:\n        util.run(\"dd if=/dev/urandom bs=32 count=1 \u003e\"+str(jwt_key), shell=True)\n\n    util.run(f\"chown -R slurm:slurm {jwt_key}\")\n    jwt_key.chmod(0o400)\n\n\ndef setup_slurmd_cronjob():\n    \"\"\" Create cronjob for keeping slurmd service up \"\"\"\n    util.run(\n        \"crontab -u root -\", input=(\n            \"*/2 * * * * \"\n            \"if [ `systemctl status slurmd | grep -c inactive` -gt 0 ]; then \"\n            \"mount -a; \"\n            \"systemctl restart munge; \"\n            \"systemctl restart slurmd; \"\n            \"fi\\n\"\n        ))\n# END setup_slurmd_cronjob()\n\n\ndef setup_nss_slurm():\n    \"\"\" install and configure nss_slurm \"\"\"\n    # setup nss_slurm\n    Path('/var/spool/slurmd').mkdirp()\n    util.run(\"ln -s {}/lib/libnss_slurm.so.2 /usr/lib64/libnss_slurm.so.2\"\n             .format(dirs.prefix))\n    util.run(\n        r\"sed -i 's/\\(^\\(passwd\\|group\\):\\s\\+\\)/\\1slurm /g' /etc/nsswitch.conf\"\n    )\n# END setup_nss_slurm()\n\n\ndef configure_dirs():\n\n    for p in dirs.values():\n        p.mkdirp()\n    shutil.chown(dirs.slurm, user='slurm', group='slurm')\n    shutil.chown(dirs.scripts, user='slurm', group='slurm')\n\n    for p in slurmdirs.values():\n        p.mkdirp()\n        shutil.chown(p, user='slurm', group='slurm')\n\n    (dirs.scripts/'etc').symlink_to(slurmdirs.etc)\n    shutil.chown(dirs.scripts/'etc', user='slurm', group='slurm')\n\n    (dirs.scripts/'log').symlink_to(slurmdirs.log)\n    shutil.chown(dirs.scripts/'log', user='slurm', group='slurm')\n\n\ndef setup_controller():\n    \"\"\" Run controller setup \"\"\"\n    expand_instance_templates()\n    install_cgroup_conf()\n    install_slurm_conf()\n    install_slurmdbd_conf()\n    setup_jwt_key()\n    util.run(\"create-munge-key -f\")\n    util.run(\"systemctl restart munge\")\n\n    if cfg.controller_secondary_disk:\n        setup_secondary_disks()\n    setup_network_storage()\n    mount_fstab()\n\n    try:\n        util.run(str(dirs.scripts/'custom-controller-install'))\n    except Exception:\n        # Ignore blank files with no shell magic.\n        pass\n\n    if not cfg.cloudsql:\n        cnfdir = Path('/etc/my.cnf.d')\n        if not cnfdir.exists():\n            cnfdir = Path('/etc/mysql/conf.d')\n        (cnfdir/'mysql_slurm.cnf').write_text(\"\"\"\n[mysqld]\nbind-address = 127.0.0.1\n\"\"\")\n        util.run('systemctl enable mariadb')\n        util.run('systemctl start mariadb')\n\n        mysql = \"mysql -u root -e\"\n        util.run(\n            f\"\"\"{mysql} \"create user 'slurm'@'localhost'\";\"\"\")\n        util.run(\n            f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'localhost'\";\"\"\")\n        util.run(\n            f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'{CONTROL_MACHINE}'\";\"\"\")\n\n    util.run(\"systemctl enable slurmdbd\")\n    util.run(\"systemctl start slurmdbd\")\n\n    # Wait for slurmdbd to come up\n    time.sleep(5)\n\n    sacctmgr = f\"{dirs.prefix}/bin/sacctmgr -i\"\n    util.run(f\"{sacctmgr} add cluster {cfg.cluster_name}\")\n\n    util.run(\"systemctl enable slurmctld\")\n    util.run(\"systemctl start slurmctld\")\n\n    util.run(\"systemctl enable slurmrestd\")\n    util.run(\"systemctl start slurmrestd\")\n\n    # Export at the end to signal that everything is up\n    util.run(\"systemctl enable nfs-server\")\n    util.run(\"systemctl start nfs-server\")\n\n    setup_nfs_exports()\n    setup_sync_cronjob()\n\n    log.info(\"Done setting up controller\")\n    pass\n\n\ndef setup_login():\n    \"\"\" run login node setup \"\"\"\n    setup_network_storage()\n    mount_fstab()\n    util.run(\"systemctl restart munge\")\n\n    try:\n        util.run(str(dirs.scripts/'custom-compute-install'))\n    except Exception:\n        # Ignore blank files with no shell magic.\n        pass\n    log.info(\"Done setting up login\")\n\n\ndef setup_compute():\n    \"\"\" run compute node setup \"\"\"\n    setup_nss_slurm()\n    setup_network_storage()\n    mount_fstab()\n\n    pid = util.get_pid(cfg.hostname)\n    if cfg.instance_defs[pid].gpu_count:\n        retries = n = 50\n        while util.run(\"nvidia-smi\").returncode != 0 and n \u003e 0:\n            n -= 1\n            log.info(f\"Nvidia driver not yet loaded, try {retries-n}\")\n            time.sleep(5)\n\n    try:\n        util.run(str(dirs.scripts/'custom-compute-install'))\n    except Exception:\n        # Ignore blank files with no shell magic.\n        pass\n\n    setup_slurmd_cronjob()\n    util.run(\"systemctl restart munge\")\n    util.run(\"systemctl enable slurmd\")\n    util.run(\"systemctl start slurmd\")\n\n    log.info(\"Done setting up compute\")\n\n\ndef main():\n\n    start_motd()\n    configure_dirs()\n    install_meta_files()\n\n    # call the setup function for the instance type\n    setup = dict.get(\n        {\n            'controller': setup_controller,\n            'compute': setup_compute,\n            'login': setup_login\n        },\n        cfg.instance_type,\n        lambda: log.fatal(f\"Unknown instance type: {cfg.instance_type}\")\n    )\n    setup()\n\n    end_motd()\n# END main()\n\n\nif __name__ == '__main__':\n    main()\n",
              "slurm-resume": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n# Modified for use with the Slurm Resource Manager.\n#\n# Copyright 2015 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport httplib2\nimport logging\nimport os\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom itertools import groupby, chain\n\nimport googleapiclient.discovery\nfrom google.auth import compute_engine\nimport google_auth_httplib2\nfrom googleapiclient.http import set_user_agent\n\nimport util\n\nPLACEMENT_MAX_CNT = 22\n\ncfg = util.Config.load_config(Path(__file__).with_name('config.yaml'))\n\nSCONTROL = Path(cfg.slurm_cmd_path or '')/'scontrol'\nLOGFILE = (Path(cfg.log_dir or '')/Path(__file__).name).with_suffix('.log')\nSCRIPTS_DIR = Path(__file__).parent.resolve()\n\nTOT_REQ_CNT = 1000\n\ninstances = {}\n\nif cfg.google_app_cred_path:\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cfg.google_app_cred_path\n\n\ndef create_instance(compute, instance_def, node_list, placement_group_name):\n\n    # Configure the machine\n\n    meta_files = {\n        'config': SCRIPTS_DIR/'config.yaml',\n        'util-script': SCRIPTS_DIR/'util.py',\n        'startup-script': SCRIPTS_DIR/'startup.sh',\n        'setup-script': SCRIPTS_DIR/'setup.py',\n    }\n    custom_compute = SCRIPTS_DIR/'custom-compute-install'\n    if custom_compute.exists():\n        meta_files['custom-compute-install'] = str(custom_compute)\n\n    config = {\n        'name': 'notused',\n\n        # Specify a network interface\n        'networkInterfaces': [{\n            'subnetwork': (\n                \"projects/{}/regions/{}/subnetworks/{}\".format(\n                    cfg.shared_vpc_host_project or cfg.project,\n                    instance_def.region,\n                    (instance_def.vpc_subnet\n                     or f'{cfg.cluster_name}-{instance_def.region}'))\n            ),\n        }],\n\n        'tags': {'items': ['compute']},\n\n        'metadata': {\n            'items': [\n                {'key': 'enable-oslogin',\n                 'value': 'TRUE'},\n                {'key': 'VmDnsSetting',\n                 'value': 'GlobalOnly'},\n                *[{'key': k, 'value': Path(v).read_text()} for k, v in meta_files.items()]\n            ]\n        }\n    }\n\n    if instance_def.machine_type:\n        config['machineType'] = instance_def.machine_type\n\n    if (instance_def.image and\n            instance_def.compute_disk_type and\n            instance_def.compute_disk_size_gb):\n        config['disks'] = [{\n            'boot': True,\n            'autoDelete': True,\n            'initializeParams': {\n                'sourceImage': instance_def.image,\n                'diskType': instance_def.compute_disk_type,\n                'diskSizeGb': instance_def.compute_disk_size_gb\n            }\n        }]\n\n    if cfg.compute_node_service_account and cfg.compute_node_scopes:\n        # Allow the instance to access cloud storage and logging.\n        config['serviceAccounts'] = [{\n            'email': cfg.compute_node_service_account,\n            'scopes': cfg.compute_node_scopes\n        }]\n\n    if placement_group_name is not None:\n        config['scheduling'] = {\n            'onHostMaintenance': 'TERMINATE',\n            'automaticRestart': False\n        }\n        config['resourcePolicies'] = [placement_group_name]\n\n    if instance_def.gpu_count:\n        config['guestAccelerators'] = [{\n            'acceleratorCount': instance_def.gpu_count,\n            'acceleratorType': instance_def.gpu_type\n        }]\n        config['scheduling'] = {'onHostMaintenance': 'TERMINATE'}\n\n    if instance_def.preemptible_bursting:\n        config['scheduling'] = {\n            'preemptible': True,\n            'onHostMaintenance': 'TERMINATE',\n            'automaticRestart': False\n        }\n\n    if instance_def.compute_labels:\n        config['labels'] = instance_def.compute_labels\n\n    if instance_def.cpu_platform:\n        config['minCpuPlatform'] = instance_def.cpu_platform\n\n    if cfg.external_compute_ips:\n        config['networkInterfaces'][0]['accessConfigs'] = [\n            {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}\n        ]\n\n    perInstanceProperties = {k: {} for k in node_list}\n    body = {\n        'count': len(node_list),\n        'instanceProperties': config,\n        'perInstanceProperties': perInstanceProperties,\n    }\n\n    if instance_def.instance_template:\n        body['sourceInstanceTemplate'] = (\n            \"projects/{}/global/instanceTemplates/{}\".format(\n                cfg.project, instance_def.instance_template)\n        )\n\n    # For non-exclusive requests, create as many instances as possible as the\n    # nodelist isn't tied to a specific set of instances.\n    if not instance_def.exclusive:\n        body['minCount'] = 1\n\n    if instance_def.regional_capacity:\n        if instance_def.regional_policy:\n            body['locationPolicy'] = instance_def.regional_policy\n        op = compute.regionInstances().bulkInsert(\n            project=cfg.project, region=instance_def.region,\n            body=body)\n        return op.execute()\n\n    return util.ensure_execute(compute.instances().bulkInsert(\n        project=cfg.project, zone=instance_def.zone, body=body))\n# [END create_instance]\n\n\ndef add_instances(node_chunk):\n\n    node_list = node_chunk['nodes']\n    pg_name = None\n    if 'pg' in node_chunk:\n        pg_name = node_chunk['pg']\n    log.debug(f\"node_list:{node_list} pg:{pg_name}\")\n\n    auth_http = None\n    if not cfg.google_app_cred_path:\n        http = set_user_agent(httplib2.Http(),\n                              \"Slurm_GCP_Scripts/1.2 (GPN:SchedMD)\")\n        creds = compute_engine.Credentials()\n        auth_http = google_auth_httplib2.AuthorizedHttp(creds, http=http)\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              http=auth_http,\n                                              cache_discovery=False)\n    pid = util.get_pid(node_list[0])\n    instance_def = cfg.instance_defs[pid]\n\n    try:\n        operation = create_instance(compute, instance_def, node_list, pg_name)\n    except googleapiclient.errors.HttpError as e:\n        log.error(f\"failed to add {node_list[0]}*{len(node_list)} to slurm, {e}\")\n        if instance_def.exclusive:\n            os._exit(1)\n        down_nodes(node_list, e)\n        return\n\n    result = util.wait_for_operation(compute, cfg.project, operation)\n    if not result or 'error' in result:\n        grp_err_msg = result['error']['errors'][0]['message']\n        log.error(f\"group operation failed: {grp_err_msg}\")\n        if instance_def.exclusive:\n            os._exit(1)\n\n        group_ops = util.get_group_operations(compute, cfg.project, result)\n        failed_nodes = {}\n        for op in group_ops['items']:\n            if op['operationType'] != 'insert':\n                continue\n            if 'error' in op:\n                err_msg = op['error']['errors'][0]['message']\n                failed_node = op['targetLink'].split('/')[-1]\n                if err_msg not in failed_nodes:\n                    failed_nodes[err_msg] = [failed_node]\n                else:\n                    failed_nodes[err_msg].append(failed_node)\n        if failed_nodes:\n            log.error(f\"insert requests failed: {failed_nodes}\")\n            for msg, nodes in failed_nodes.items():\n                down_nodes(nodes, msg)\n\n# [END add_instances]\n\n\ndef down_nodes(node_list, reason):\n    \"\"\" set nodes in node_list down with given reason \"\"\"\n    with tempfile.NamedTemporaryFile(mode='w+t') as f:\n        f.writelines(\"\\n\".join(node_list))\n        f.flush()\n        hostlist = util.run(f\"{SCONTROL} show hostlist {f.name}\",\n                            check=True, get_stdout=True).stdout.rstrip()\n    util.run(\n        f\"{SCONTROL} update nodename={hostlist} state=down reason='{reason}'\")\n# [END down_nodes]\n\n\ndef hold_job(job_id, reason):\n    \"\"\" hold job_id \"\"\"\n    util.run(f\"{SCONTROL} hold jobid={job_id}\")\n    util.run(f\"{SCONTROL} update jobid={job_id} comment='{reason}'\")\n# [END hold_job]\n\n\ndef create_placement_groups(arg_job_id, vm_count, region):\n    log.debug(f\"Creating PG: {arg_job_id} vm_count:{vm_count} region:{region}\")\n\n    pg_names = []\n    pg_ops = []\n    pg_index = 0\n\n    auth_http = None\n    if not cfg.google_app_cred_path:\n        http = set_user_agent(httplib2.Http(),\n                              \"Slurm_GCP_Scripts/1.2 (GPN:SchedMD)\")\n        creds = compute_engine.Credentials()\n        auth_http = google_auth_httplib2.AuthorizedHttp(creds, http=http)\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              http=auth_http,\n                                              cache_discovery=False)\n\n    for i in range(vm_count):\n        if i % PLACEMENT_MAX_CNT:\n            continue\n        pg_index += 1\n        pg_name = f'{cfg.cluster_name}-{arg_job_id}-{pg_index}'\n        pg_names.append(pg_name)\n\n        config = {\n            'name': pg_name,\n            'region': region,\n            'groupPlacementPolicy': {\n                \"collocation\": \"COLLOCATED\",\n                \"vmCount\": min(vm_count - i, PLACEMENT_MAX_CNT)\n             }\n        }\n\n        pg_ops.append(util.ensure_execute(\n            compute.resourcePolicies().insert(\n                project=cfg.project, region=region, body=config)))\n\n    for operation in pg_ops:\n        result = util.wait_for_operation(compute, cfg.project, operation)\n        if result and 'error' in result:\n            err_msg = result['error']['errors'][0]['message']\n            log.error(f\" placement group operation failed: {err_msg}\")\n            os._exit(1)\n\n    return pg_names\n# [END create_placement_groups]\n\n\ndef main(arg_nodes, arg_job_id):\n    log.debug(f\"Bursting out: {arg_nodes} {arg_job_id}\")\n    # Get node list\n    nodes_str = util.run(f\"{SCONTROL} show hostnames {arg_nodes}\",\n                         check=True, get_stdout=True).stdout\n    node_list = sorted(nodes_str.splitlines(), key=util.get_pid)\n\n    placement_groups = None\n    pid = util.get_pid(node_list[0])\n    if (arg_job_id and not cfg.instance_defs[pid].exclusive):\n        # Don't create from calls by PrologSlurmctld\n        return\n\n    nodes_by_pid = {k: tuple(nodes)\n                    for k, nodes in groupby(node_list, util.get_pid)}\n\n    if not arg_job_id:\n        for pid in [pid for pid in nodes_by_pid\n                    if cfg.instance_defs[pid].exclusive]:\n            # Node was created by PrologSlurmctld, skip for ResumeProgram.\n            del nodes_by_pid[pid]\n\n    if (arg_job_id and\n            cfg.instance_defs[pid].enable_placement):\n        if cfg.instance_defs[pid].machine_type.split('-')[0] != \"c2\":\n            msg = \"Unsupported placement policy configuration. Please utilize c2 machine type.\"\n            log.error(msg)\n            hold_job(arg_job_id, msg)\n            os._exit(1)\n\n        elif len(node_list) \u003e 1:\n            log.debug(f\"creating placement group for {arg_job_id}\")\n            placement_groups = create_placement_groups(\n                arg_job_id, len(node_list), cfg.instance_defs[pid].region)\n\n    def chunks(lst, pg_names):\n        \"\"\" group list into chunks of max size n \"\"\"\n        n = 1000\n        if pg_names:\n            n = PLACEMENT_MAX_CNT\n\n        pg_index = 0\n        for i in range(0, len(lst), n):\n            chunk = dict(nodes=lst[i:i+n])\n            if pg_names:\n                chunk['pg'] = pg_names[pg_index]\n                pg_index += 1\n            yield chunk\n    # concurrently add nodes grouped by instance_def (pid), max 1000\n    with ThreadPoolExecutor() as exe:\n        node_chunks = chain.from_iterable(\n            map(partial(chunks, pg_names=placement_groups),\n                nodes_by_pid.values()))\n        exe.map(add_instances, node_chunks)\n\n    log.info(f\"done adding instances: {arg_nodes} {arg_job_id}\")\n# [END main]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('args', nargs='+', help=\"nodes [jobid]\")\n    parser.add_argument('--debug', '-d', dest='debug', action='store_true',\n                        help='Enable debugging output')\n\n    job_id = 0\n    nodes = \"\"\n\n    if \"SLURM_JOB_NODELIST\" in os.environ:\n        args = parser.parse_args(sys.argv[1:] +\n                                 [os.environ['SLURM_JOB_NODELIST'],\n                                  os.environ['SLURM_JOB_ID']])\n    else:\n        args = parser.parse_args()\n\n    nodes = args.args[0]\n    if len(args.args) \u003e 1:\n        job_id = args.args[1]\n\n    if args.debug:\n        util.config_root_logger(level='DEBUG', util_level='DEBUG',\n                                logfile=LOGFILE)\n    else:\n        util.config_root_logger(level='INFO', util_level='ERROR',\n                                logfile=LOGFILE)\n    log = logging.getLogger(Path(__file__).name)\n    sys.excepthook = util.handle_exception\n\n    new_yaml = Path(__file__).with_name('config.yaml.new')\n    if (not cfg.instance_defs or cfg.partitions) and not new_yaml.exists():\n        log.info(f\"partition declarations in config.yaml have been converted to a new format and saved to {new_yaml}. Replace config.yaml as soon as possible.\")\n        cfg.save_config(new_yaml)\n\n    main(nodes, job_id)\n",
              "slurm-suspend": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n# Modified for use with the Slurm Resource Manager.\n#\n# Copyright 2015 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport logging\nimport os\nimport sys\nimport time\nfrom itertools import groupby\nfrom pathlib import Path\n\nimport googleapiclient.discovery\n\nimport util\n\ncfg = util.Config.load_config(Path(__file__).with_name('config.yaml'))\n\nSCONTROL = Path(cfg.slurm_cmd_path or '')/'scontrol'\nLOGFILE = (Path(cfg.log_dir or '')/Path(__file__).name).with_suffix('.log')\n\nTOT_REQ_CNT = 1000\n\noperations = {}\nretry_list = []\n\nif cfg.google_app_cred_path:\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cfg.google_app_cred_path\n\n\ndef delete_instances_cb(request_id, response, exception):\n    if exception is not None:\n        log.error(f\"delete exception for node {request_id}: {exception}\")\n        if \"Rate Limit Exceeded\" in str(exception):\n            retry_list.append(request_id)\n    else:\n        operations[request_id] = response\n# [END delete_instances_cb]\n\n\ndef delete_instances(compute, node_list, arg_job_id):\n\n    batch_list = []\n    curr_batch = 0\n    req_cnt = 0\n    batch_list.insert(\n        curr_batch,\n        compute.new_batch_http_request(callback=delete_instances_cb))\n\n    def_list = {pid: cfg.instance_defs[pid]\n                for pid, nodes in groupby(node_list, util.get_pid)}\n    regional_instances = util.get_regional_instances(compute, cfg.project,\n                                                     def_list)\n\n    for node_name in node_list:\n\n        pid = util.get_pid(node_name)\n        if (not arg_job_id and cfg.instance_defs[pid].exclusive):\n            # Node was deleted by EpilogSlurmctld, skip for SuspendProgram\n            continue\n\n        zone = None\n        if cfg.instance_defs[pid].regional_capacity:\n            instance = regional_instances.get(node_name, None)\n            if instance is None:\n                log.debug(\"Regional node not found. Already deleted?\")\n                continue\n            zone = instance['zone'].split('/')[-1]\n        else:\n            zone = cfg.instance_defs[pid].zone\n\n        if req_cnt \u003e= TOT_REQ_CNT:\n            req_cnt = 0\n            curr_batch += 1\n            batch_list.insert(\n                curr_batch,\n                compute.new_batch_http_request(callback=delete_instances_cb))\n\n        batch_list[curr_batch].add(\n            compute.instances().delete(project=cfg.project,\n                                       zone=zone,\n                                       instance=node_name),\n            request_id=node_name)\n        req_cnt += 1\n\n    try:\n        for i, batch in enumerate(batch_list):\n            util.ensure_execute(batch)\n            if i \u003c (len(batch_list) - 1):\n                time.sleep(30)\n    except Exception:\n        log.exception(\"error in batch:\")\n\n# [END delete_instances]\n\n\ndef delete_placement_groups(compute, node_list, arg_job_id):\n    PLACEMENT_MAX_CNT = 22\n    pg_ops = []\n    pg_index = 0\n    pid = util.get_pid(node_list[0])\n\n    for i in range(len(node_list)):\n        if i % PLACEMENT_MAX_CNT:\n            continue\n        pg_index += 1\n        pg_name = f'{cfg.cluster_name}-{arg_job_id}-{pg_index}'\n        pg_ops.append(compute.resourcePolicies().delete(\n            project=cfg.project, region=cfg.instance_defs[pid].region,\n            resourcePolicy=pg_name).execute())\n    for operation in pg_ops:\n        util.wait_for_operation(compute, cfg.project, operation)\n    log.debug(\"done deleting pg\")\n# [END delete_placement_groups]\n\n\ndef main(arg_nodes, arg_job_id):\n    log.debug(f\"deleting nodes:{arg_nodes} job_id:{job_id}\")\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n\n    # Get node list\n    nodes_str = util.run(f\"{SCONTROL} show hostnames {arg_nodes}\",\n                         check=True, get_stdout=True).stdout\n    node_list = nodes_str.splitlines()\n\n    pid = util.get_pid(node_list[0])\n    if (arg_job_id and not cfg.instance_defs[pid].exclusive):\n        # Don't delete from calls by EpilogSlurmctld\n        return\n\n    if arg_job_id:\n        # Mark nodes as off limits to new jobs while powering down.\n        # Have to use \"down\" because it's the only, current, way to remove the\n        # power_up flag from the node -- followed by a power_down -- if the\n        # PrologSlurmctld fails with a non-zero exit code.\n        util.run(\n            f\"{SCONTROL} update node={arg_nodes} state=down reason='{arg_job_id} finishing'\")\n        # Power down nodes in slurm, so that they will become available again.\n        util.run(\n            f\"{SCONTROL} update node={arg_nodes} state=power_down\")\n\n    while True:\n        delete_instances(compute, node_list, arg_job_id)\n        if not len(retry_list):\n            break\n\n        log.debug(\"got {} nodes to retry ({})\"\n                  .format(len(retry_list), ','.join(retry_list)))\n        node_list = list(retry_list)\n        del retry_list[:]\n\n    if arg_job_id:\n        for operation in operations.values():\n            try:\n                util.wait_for_operation(compute, cfg.project, operation)\n            except Exception:\n                log.exception(f\"Error in deleting {operation['name']} to slurm\")\n\n    log.debug(\"done deleting instances\")\n\n    if (arg_job_id and\n            cfg.instance_defs[pid].enable_placement and\n            cfg.instance_defs[pid].machine_type.split('-')[0] == \"c2\" and\n            len(node_list) \u003e 1):\n        delete_placement_groups(compute, node_list, arg_job_id)\n\n    log.info(f\"done deleting nodes:{arg_nodes} job_id:{job_id}\")\n\n# [END main]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('args', nargs='+', help=\"nodes [jobid]\")\n    parser.add_argument('--debug', '-d', dest='debug', action='store_true',\n                        help='Enable debugging output')\n\n    if \"SLURM_JOB_NODELIST\" in os.environ:\n        args = parser.parse_args(sys.argv[1:] +\n                                 [os.environ['SLURM_JOB_NODELIST'],\n                                  os.environ['SLURM_JOB_ID']])\n    else:\n        args = parser.parse_args()\n\n    nodes = args.args[0]\n    job_id = 0\n    if len(args.args) \u003e 1:\n        job_id = args.args[1]\n\n    if args.debug:\n        util.config_root_logger(level='DEBUG', util_level='DEBUG',\n                                logfile=LOGFILE)\n    else:\n        util.config_root_logger(level='INFO', util_level='ERROR',\n                                logfile=LOGFILE)\n    log = logging.getLogger(Path(__file__).name)\n    sys.excepthook = util.handle_exception\n\n    main(nodes, job_id)\n",
              "slurm_conf_tpl": "# slurm.conf file generated by configurator.html.\n# Put this file on all nodes of your cluster.\n# See the slurm.conf man page for more information.\n#\nControlMachine={control_host}\n#ControlAddr=\n#BackupController=\n#BackupAddr=\n#\nAuthType=auth/munge\nAuthInfo=cred_expire=120\nAuthAltTypes=auth/jwt\n#CheckpointType=checkpoint/none\nCredType=cred/munge\n#DisableRootJobs=NO\n#EnforcePartLimits=NO\n#Epilog=\n#FirstJobId=1\n#MaxJobId=999999\n#GroupUpdateForce=0\n#GroupUpdateTime=600\n#JobCheckpointDir=/var/slurm/checkpoint\n#JobCredentialPrivateKey=\n#JobCredentialPublicCertificate=\n#JobFileAppend=0\n#JobRequeue=1\n#JobSubmitPlugins=1\n#KillOnBadExit=0\n#LaunchType=launch/slurm\n#Licenses=foo*4,bar\n#MailProg=/bin/mail\n#MaxJobCount=5000\n#MaxStepCount=40000\n#MaxTasksPerNode=128\nMpiDefault={mpi_default}\n#MpiParams=ports=#-#\n#PluginDir=\n#PlugStackConfig=\n#PrivateData=jobs\nLaunchParameters=enable_nss_slurm,use_interactive_step\n\n# Always show cloud nodes. Otherwise cloud nodes are hidden until they are\n# resumed. Having them shown can be useful in detecting downed nodes.\n# NOTE: slurm won't allocate/resume nodes that are down. So in the case of\n# preemptible nodes -- if gcp preempts a node, the node will eventually be put\n# into a down date because the node will stop responding to the controller.\n# (e.g. SlurmdTimeout).\nPrivateData=cloud\n\nProctrackType=proctrack/cgroup\n\n#Prolog=\n#PrologFlags=\n#PropagatePrioProcess=0\n#PropagateResourceLimits=\n#PropagateResourceLimitsExcept=Sched\n#RebootProgram=\n\nReturnToService=2\n#SallocDefaultCommand=\nSlurmctldPidFile=/var/run/slurm/slurmctld.pid\nSlurmctldPort=6820-6830\nSlurmdPidFile=/var/run/slurm/slurmd.pid\nSlurmdPort=6818\nSlurmdSpoolDir=/var/spool/slurmd\nSlurmUser=slurm\n#SlurmdUser=root\n#SrunEpilog=\n#SrunProlog=\nStateSaveLocation={state_save}\nSwitchType=switch/none\n#TaskEpilog=\nTaskPlugin=task/affinity,task/cgroup\n#TaskPluginParam=\n#TaskProlog=\n#TopologyPlugin=topology/tree\n#TmpFS=/tmp\n#TrackWCKey=no\n#TreeWidth=\n#UnkillableStepProgram=\n#UsePAM=0\n#\n#\n# TIMERS\n#BatchStartTimeout=10\n#CompleteWait=0\n#EpilogMsgTime=2000\n#GetEnvTimeout=2\n#HealthCheckInterval=0\n#HealthCheckProgram=\nInactiveLimit=0\nKillWait=30\nMessageTimeout=60\n#ResvOverRun=0\nMinJobAge=300\n#OverTimeLimit=0\nSlurmctldTimeout=120\nSlurmdTimeout=300\n#UnkillableStepTimeout=60\n#VSizeFactor=0\nWaittime=0\n#\n#\n# SCHEDULING\n#MaxMemPerCPU=0\n#SchedulerTimeSlice=30\nSchedulerType=sched/backfill\nSelectType=select/cons_tres\nSelectTypeParameters=CR_Core_Memory\n#\n#\n# JOB PRIORITY\n#PriorityFlags=\n#PriorityType=priority/basic\n#PriorityDecayHalfLife=\n#PriorityCalcPeriod=\n#PriorityFavorSmall=\n#PriorityMaxAge=\n#PriorityUsageResetPeriod=\n#PriorityWeightAge=\n#PriorityWeightFairshare=\n#PriorityWeightJobSize=\n#PriorityWeightPartition=\n#PriorityWeightQOS=\n#\n#\n# LOGGING AND ACCOUNTING\n#AccountingStorageEnforce=associations,limits,qos,safe\nAccountingStorageHost={control_host}\n#AccountingStorageLoc=\n#AccountingStoragePass=\n#AccountingStoragePort=\nAccountingStorageType=accounting_storage/slurmdbd\n#AccountingStorageUser=\nAccountingStoreJobComment=YES\nClusterName={name}\n#DebugFlags=powersave\n#JobCompHost=\n#JobCompLoc=\n#JobCompPass=\n#JobCompPort=\nJobCompType=jobcomp/none\n#JobCompUser=\n#JobContainerType=job_container/none\nJobAcctGatherFrequency=30\nJobAcctGatherType=jobacct_gather/linux\nSlurmctldDebug=info\nSlurmctldLogFile={slurmlog}/slurmctld.log\nSlurmdDebug=info\nSlurmdLogFile={slurmlog}/slurmd-%n.log\n#\n#\n\n# Use Prolog/EpilogSlurmctld to make job to node one-to-one.\n# enable_placement=true with c2-standards creates placement groups\n# enable_placement=true w/out c2-standards creates one-to-one mappings of nodes.\n# must set OverSubscribe=Exclusive on the corresponding partitions.\nPrologSlurmctld={scripts}/resume.py\nEpilogSlurmctld={scripts}/suspend.py\n\n# POWER SAVE SUPPORT FOR IDLE NODES (optional)\nSuspendProgram={scripts}/suspend.py\nResumeProgram={scripts}/resume.py\nResumeFailProgram={scripts}/suspend.py\nSuspendTimeout={suspend_timeout}\nResumeTimeout={resume_timeout}\nResumeRate=0\n#SuspendExcNodes=\n#SuspendExcParts=\nSuspendRate=0\nSuspendTime={suspend_time}\n#\nSchedulerParameters=salloc_wait_nodes\nSlurmctldParameters=cloud_dns,idle_on_node_suspend\nCommunicationParameters=NoAddrCache\nGresTypes=gpu\n#\n# COMPUTE NODES\n",
              "slurmdbd_conf_tpl": "#ArchiveEvents=yes\n#ArchiveJobs=yes\n#ArchiveResvs=yes\n#ArchiveSteps=no\n#ArchiveSuspend=no\n#ArchiveTXN=no\n#ArchiveUsage=no\n\nAuthType=auth/munge\nAuthAltTypes=auth/jwt\nAuthAltParameters=jwt_key={state_save}/jwt_hs256.key\n\nDbdHost={control_host}\nDebugLevel=debug\n\n#PurgeEventAfter=1month\n#PurgeJobAfter=12month\n#PurgeResvAfter=1month\n#PurgeStepAfter=1month\n#PurgeSuspendAfter=1month\n#PurgeTXNAfter=12month\n#PurgeUsageAfter=24month\n\nLogFile={slurmlog}/slurmdbd.log\nPidFile=/var/run/slurm/slurmdbd.pid\n\nSlurmUser=slurm\n\nStorageLoc={db_name}\n\nStorageType=accounting_storage/mysql\nStorageHost={db_host}\nStoragePort={db_port}\nStorageUser={db_user}\nStoragePass={db_pass}\n#StorageUser=database_mgr\n#StoragePass=shazaam\n",
              "slurmsync": "#!/usr/bin/env python3\n\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport collections\nimport fcntl\nimport logging\nimport os\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\n\nimport googleapiclient.discovery\n\nimport util\n\n\ncfg = util.Config.load_config(Path(__file__).with_name('config.yaml'))\n\nSCONTROL = Path(cfg.slurm_cmd_path or '')/'scontrol'\nLOGFILE = (Path(cfg.log_dir or '')/Path(__file__).name).with_suffix('.log')\nSCRIPTS_DIR = Path(__file__).parent.resolve()\n\nTOT_REQ_CNT = 1000\n\nretry_list = []\n\nif cfg.google_app_cred_path:\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cfg.google_app_cred_path\n\n\ndef start_instances_cb(request_id, response, exception):\n    if exception is not None:\n        log.error(\"start exception: \" + str(exception))\n        if \"Rate Limit Exceeded\" in str(exception):\n            retry_list.append(request_id)\n        elif \"was not found\" in str(exception):\n            util.spawn(f\"{SCRIPTS_DIR}/resume.py {request_id}\")\n# [END start_instances_cb]\n\n\ndef start_instances(compute, node_list, gcp_nodes):\n\n    req_cnt = 0\n    curr_batch = 0\n    batch_list = []\n    batch_list.insert(\n        curr_batch,\n        compute.new_batch_http_request(callback=start_instances_cb))\n\n    for node in node_list:\n\n        pid = util.get_pid(node)\n        zone = cfg.instance_defs[pid].zone\n\n        if cfg.instance_defs[pid].regional_capacity:\n            g_node = gcp_nodes.get(node, None)\n            if not g_node:\n                log.error(f\"Didn't find regional GCP record for '{node}'\")\n                continue\n            zone = g_node['zone'].split('/')[-1]\n\n        if req_cnt \u003e= TOT_REQ_CNT:\n            req_cnt = 0\n            curr_batch += 1\n            batch_list.insert(\n                curr_batch,\n                compute.new_batch_http_request(callback=start_instances_cb))\n\n        batch_list[curr_batch].add(\n            compute.instances().start(project=cfg.project, zone=zone,\n                                      instance=node),\n            request_id=node)\n        req_cnt += 1\n    try:\n        for i, batch in enumerate(batch_list):\n            util.ensure_execute(batch)\n            if i \u003c (len(batch_list) - 1):\n                time.sleep(30)\n    except Exception:\n        log.exception(\"error in start batch: \")\n\n# [END start_instances]\n\n\ndef main():\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n\n    try:\n        s_nodes = dict()\n        cmd = (f\"{SCONTROL} show nodes | \"\n               r\"grep -oP '^NodeName=\\K(\\S+)|State=\\K(\\S+)' | \"\n               \"paste -sd',\\n'\")\n        nodes = util.run(cmd, shell=True, check=True, get_stdout=True).stdout\n        if nodes:\n            # result is a list of tuples like:\n            # (nodename, (base='base_state', flags=\u003cset of state flags\u003e))\n            # from 'nodename,base_state+flag1+flag2'\n            # state flags include: CLOUD, COMPLETING, DRAIN, FAIL, POWER,\n            #   POWERING_DOWN\n            # Modifiers on base state still include: @ (reboot), $ (maint),\n            #   * (nonresponsive), # (powering up)\n            StateTuple = collections.namedtuple('StateTuple', 'base,flags')\n\n            def make_state_tuple(state):\n                return StateTuple(state[0], set(state[1:]))\n            s_nodes = [(node, make_state_tuple(args.split('+')))\n                       for node, args in\n                       map(lambda x: x.split(','), nodes.rstrip().splitlines())\n                       if 'CLOUD' in args]\n\n        g_nodes = util.get_regional_instances(compute, cfg.project,\n                                              cfg.instance_defs)\n        for pid, part in cfg.instance_defs.items():\n            page_token = \"\"\n            while True:\n                if not part.regional_capacity:\n                    resp = util.ensure_execute(\n                        compute.instances().list(\n                            project=cfg.project, zone=part.zone,\n                            fields='items(name,zone,status),nextPageToken',\n                            pageToken=page_token, filter=f\"name={pid}-*\"))\n\n                    if \"items\" in resp:\n                        g_nodes.update({instance['name']: instance\n                                       for instance in resp['items']})\n                    if \"nextPageToken\" in resp:\n                        page_token = resp['nextPageToken']\n                        continue\n\n                break\n\n        to_down = []\n        to_idle = []\n        to_start = []\n        for s_node, s_state in s_nodes:\n            g_node = g_nodes.get(s_node, None)\n            pid = util.get_pid(s_node)\n\n            if (('POWER' not in s_state.flags) and\n                    ('POWERING_DOWN' not in s_state.flags)):\n                # slurm nodes that aren't in power_save and are stopped in GCP:\n                #   mark down in slurm\n                #   start them in gcp\n                if g_node and (g_node['status'] == \"TERMINATED\"):\n                    if not s_state.base.startswith('DOWN'):\n                        to_down.append(s_node)\n                    if (cfg.instance_defs[pid].preemptible_bursting):\n                        to_start.append(s_node)\n\n                # can't check if the node doesn't exist in GCP while the node\n                # is booting because it might not have been created yet by the\n                # resume script.\n                # This should catch the completing states as well.\n                if (g_node is None and \"#\" not in s_state.base and\n                        not s_state.base.startswith('DOWN')):\n                    to_down.append(s_node)\n\n            elif g_node is None:\n                # find nodes that are down~ in slurm and don't exist in gcp:\n                #   mark idle~\n                if s_state.base.startswith('DOWN') and 'POWER' in s_state.flags:\n                    to_idle.append(s_node)\n                elif 'POWERING_DOWN' in s_state.flags:\n                    to_idle.append(s_node)\n                elif s_state.base.startswith('COMPLETING'):\n                    to_down.append(s_node)\n\n        if len(to_down):\n            log.info(\"{} stopped/deleted instances ({})\".format(\n                len(to_down), \",\".join(to_down)))\n            log.info(\"{} instances to start ({})\".format(\n                len(to_start), \",\".join(to_start)))\n\n            # write hosts to a file that can be given to get a slurm\n            # hostlist. Since the number of hosts could be large.\n            tmp_file = tempfile.NamedTemporaryFile(mode='w+t', delete=False)\n            tmp_file.writelines(\"\\n\".join(to_down))\n            tmp_file.close()\n            log.debug(\"tmp_file = {}\".format(tmp_file.name))\n\n            hostlist = util.run(f\"{SCONTROL} show hostlist {tmp_file.name}\",\n                                check=True, get_stdout=True).stdout.rstrip()\n            log.debug(\"hostlist = {}\".format(hostlist))\n            os.remove(tmp_file.name)\n\n            util.run(f\"{SCONTROL} update nodename={hostlist} state=down \"\n                     \"reason='Instance stopped/deleted'\")\n\n            while True:\n                start_instances(compute, to_start, g_nodes)\n                if not len(retry_list):\n                    break\n\n                log.debug(\"got {} nodes to retry ({})\"\n                          .format(len(retry_list), ','.join(retry_list)))\n                to_start = list(retry_list)\n                del retry_list[:]\n\n        if len(to_idle):\n            log.info(\"{} instances to resume ({})\".format(\n                len(to_idle), ','.join(to_idle)))\n\n            # write hosts to a file that can be given to get a slurm\n            # hostlist. Since the number of hosts could be large.\n            tmp_file = tempfile.NamedTemporaryFile(mode='w+t', delete=False)\n            tmp_file.writelines(\"\\n\".join(to_idle))\n            tmp_file.close()\n            log.debug(\"tmp_file = {}\".format(tmp_file.name))\n\n            hostlist = util.run(f\"{SCONTROL} show hostlist {tmp_file.name}\",\n                                check=True, get_stdout=True).stdout.rstrip()\n            log.debug(\"hostlist = {}\".format(hostlist))\n            os.remove(tmp_file.name)\n\n            util.run(f\"{SCONTROL} update nodename={hostlist} state=resume\")\n\n    except Exception:\n        log.exception(\"failed to sync instances\")\n\n# [END main]\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('--debug', '-d', dest='debug', action='store_true',\n                        help='Enable debugging output')\n\n    args = parser.parse_args()\n    if args.debug:\n        util.config_root_logger(level='DEBUG', util_level='DEBUG',\n                                logfile=LOGFILE)\n    else:\n        util.config_root_logger(level='INFO', util_level='ERROR',\n                                logfile=LOGFILE)\n    log = logging.getLogger(Path(__file__).name)\n    sys.excepthook = util.handle_exception\n\n    # only run one instance at a time\n    pid_file = (Path('/tmp')/Path(__file__).name).with_suffix('.pid')\n    with pid_file.open('w') as fp:\n        try:\n            fcntl.lockf(fp, fcntl.LOCK_EX | fcntl.LOCK_NB)\n        except IOError:\n            sys.exit(0)\n\n    main()\n",
              "util-script": "#!/usr/bin/env python3\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport logging.config\nimport os\nimport shlex\nimport subprocess\nimport sys\nimport socket\nimport time\nfrom itertools import chain, compress\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\nimport googleapiclient.discovery\n\nimport requests\nimport yaml\n\n\nlog = logging.getLogger(__name__)\n\n\ndef config_root_logger(level='DEBUG', util_level=None,\n                       stdout=True, logfile=None):\n    if not util_level:\n        util_level = level\n    handlers = list(compress(('stdout_handler', 'file_handler'),\n                             (stdout, logfile)))\n\n    config = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'standard': {\n                'format': '',\n            },\n            'stamp': {\n                'format': '%(asctime)s %(process)s %(thread)s %(name)s %(levelname)s: %(message)s',\n            },\n        },\n        'handlers': {\n            'stdout_handler': {\n                'level': 'DEBUG',\n                'formatter': 'standard',\n                'class': 'logging.StreamHandler',\n                'stream': sys.stdout,\n            },\n        },\n        'loggers': {\n            __name__: {  # enable util.py logging\n                'level': util_level,\n            },\n        },\n        'root': {\n            'handlers': handlers,\n            'level': level,\n        }\n    }\n    if logfile:\n        config['handlers']['file_handler'] = {\n            'level': 'DEBUG',\n            'formatter': 'stamp',\n            'class': 'logging.handlers.WatchedFileHandler',\n            'filename': logfile,\n        }\n    logging.config.dictConfig(config)\n\n\ndef handle_exception(exc_type, exc_value, exc_trace):\n    if not issubclass(exc_type, KeyboardInterrupt):\n        log.exception(\"Fatal exception\",\n                      exc_info=(exc_type, exc_value, exc_trace))\n    sys.__excepthook__(exc_type, exc_value, exc_trace)\n\n\ndef get_metadata(path):\n    \"\"\" Get metadata relative to metadata/computeMetadata/v1/instance/ \"\"\"\n    URL = 'http://metadata.google.internal/computeMetadata/v1/instance/'\n    HEADERS = {'Metadata-Flavor': 'Google'}\n    full_path = URL + path\n    try:\n        resp = requests.get(full_path, headers=HEADERS)\n        resp.raise_for_status()\n    except requests.exceptions.RequestException:\n        log.error(f\"Error while getting metadata from {full_path}\")\n        return None\n    return resp.text\n\n\ndef run(cmd, wait=0, quiet=False, get_stdout=False,\n        shell=False, universal_newlines=True, **kwargs):\n    \"\"\" run in subprocess. Optional wait after return. \"\"\"\n    if not quiet:\n        log.debug(f\"run: {cmd}\")\n    if get_stdout:\n        kwargs['stdout'] = subprocess.PIPE\n\n    args = cmd if shell else shlex.split(cmd)\n    ret = subprocess.run(args, shell=shell,\n                         universal_newlines=universal_newlines,\n                         **kwargs)\n    if wait:\n        time.sleep(wait)\n    return ret\n\n\ndef spawn(cmd, quiet=False, shell=False, **kwargs):\n    \"\"\" nonblocking spawn of subprocess \"\"\"\n    if not quiet:\n        log.debug(f\"spawn: {cmd}\")\n    args = cmd if shell else shlex.split(cmd)\n    return subprocess.Popen(args, shell=shell, **kwargs)\n\n\ndef get_pid(node_name):\n    \"\"\"Convert \u003cprefix\u003e-\u003cpid\u003e-\u003cnid\u003e\"\"\"\n\n    return '-'.join(node_name.split('-')[:-1])\n\n\n@contextmanager\ndef cd(path):\n    \"\"\" Change working directory for context \"\"\"\n    prev = Path.cwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(prev)\n\n\ndef static_vars(**kwargs):\n    \"\"\"\n    Add variables to the function namespace.\n    @static_vars(var=init): var must be referenced func.var\n    \"\"\"\n    def decorate(func):\n        for k in kwargs:\n            setattr(func, k, kwargs[k])\n        return func\n    return decorate\n\n\nclass cached_property:\n    \"\"\"\n    Descriptor for creating a property that is computed once and cached\n    \"\"\"\n    def __init__(self, factory):\n        self._attr_name = factory.__name__\n        self._factory = factory\n\n    def __get__(self, instance, owner=None):\n        if instance is None:  # only if invoked from class\n            return self\n        attr = self._factory(instance)\n        setattr(instance, self._attr_name, attr)\n        return attr\n\n\nclass NSDict(OrderedDict):\n    \"\"\" Simple nested dict namespace \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        def from_nested(value):\n            \"\"\" If value is dict, convert to NSDict. Also recurse lists. \"\"\"\n            if isinstance(value, dict):\n                return type(self)({k: from_nested(v) for k, v in value.items()})\n            elif isinstance(value, list):\n                return [from_nested(v) for v in value]\n            else:\n                return value\n\n        super(NSDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self  # all properties are member attributes\n\n        # Convert nested elements\n        for k, v in self.items():\n            self[k] = from_nested(v)\n\n\nclass Config(NSDict):\n    \"\"\" Loads config from yaml and holds values in nested namespaces \"\"\"\n\n    TYPES = set(('compute', 'login', 'controller'))\n    # PROPERTIES defines which properties in slurm.jinja.schema are included\n    #   in the config file. SAVED_PROPS are saved to file via save_config.\n    SAVED_PROPS = ('project',\n                   'zone',\n                   'cluster_name',\n                   'external_compute_ips',\n                   'shared_vpc_host_project',\n                   'compute_node_service_account',\n                   'compute_node_scopes',\n                   'slurm_cmd_path',\n                   'log_dir',\n                   'google_app_cred_path',\n                   'update_node_addrs',\n                   'network_storage',\n                   'login_network_storage',\n                   'instance_defs',\n                   )\n    PROPERTIES = (*SAVED_PROPS,\n                  'munge_key',\n                  'jwt_key',\n                  'external_compute_ips',\n                  'controller_secondary_disk',\n                  'suspend_time',\n                  'login_node_count',\n                  'cloudsql',\n                  'partitions',\n                  )\n\n    def __init__(self, *args, **kwargs):\n        super(Config, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def new_config(cls, properties):\n        # If k is ever not found, None will be inserted as the value\n        cfg = cls({k: properties.setdefault(k, None) for k in cls.PROPERTIES})\n        if cfg.partitions:\n            cfg['instance_defs'] = Config({\n                f'{cfg.cluster_name}-compute-{pid}': part\n                for pid, part in enumerate(cfg.partitions)\n            })\n\n        for netstore in (*cfg.network_storage, *(cfg.login_network_storage or []),\n                         *chain(*(p.network_storage for p in (cfg.partitions or [])))):\n            if netstore.server_ip == '$controller':\n                netstore.server_ip = cfg.cluster_name + '-controller'\n        return cfg\n\n    @classmethod\n    def load_config(cls, path):\n        config = yaml.safe_load(Path(path).read_text())\n        return cls(config)\n\n    def save_config(self, path):\n        save_dict = Config([(k, self[k]) for k in self.SAVED_PROPS])\n        if save_dict.instance_defs:\n            for instance_type in save_dict.instance_defs.values():\n                instance_type.pop('max_node_count', 0)\n                instance_type.pop('name', 0)\n                instance_type.pop('static_node_count', 0)\n        Path(path).write_text(yaml.dump(save_dict, Dumper=Dumper))\n\n    @cached_property\n    def instance_type(self):\n        # get tags, intersect with possible types, get the first or none\n        tags = yaml.safe_load(get_metadata('tags'))\n        # TODO what to default to if no match found.\n        return next(iter(set(tags) \u0026 self.TYPES), None)\n\n    @cached_property\n    def hostname(self):\n        return socket.gethostname()\n\n    @property\n    def region(self):\n        if self.zone:\n            parts = self.zone.split('-')\n            if len(parts) \u003e 2:\n                return '-'.join(parts[:-1])\n            else:\n                return self.zone\n        return None\n\n    @property\n    def exclusive(self):\n        return bool(self.get('exclusive', False) or self.enable_placement)\n\n    def __getattr__(self, item):\n        \"\"\" only called if item is not found in self \"\"\"\n        return None\n\n\nclass Dumper(yaml.SafeDumper):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.add_representer(Config, self.represent_nsdict)\n        self.add_representer(NSDict, self.represent_nsdict)\n        self.add_multi_representer(Path, self.represent_path)\n\n    @staticmethod\n    def represent_nsdict(dumper, data):\n        return dumper.represent_mapping('tag:yaml.org,2002:map',\n                                        data.items())\n\n    @staticmethod\n    def represent_path(dumper, path):\n        return dumper.represent_scalar('tag:yaml.org,2002:str',\n                                       str(path))\n\n\ndef ensure_execute(operation):\n    \"\"\" Handle rate limits and socket time outs \"\"\"\n\n    retry = 0\n    sleep = 1\n    max_sleep = 60\n    while True:\n        try:\n            return operation.execute()\n\n        except googleapiclient.errors.HttpError as e:\n            if \"Rate Limit Exceeded\" in str(e):\n                retry += 1\n                sleep = min(sleep*2, max_sleep)\n                log.error(f\"retry:{retry} sleep:{sleep} '{e}'\")\n                time.sleep(sleep)\n                continue\n            raise\n\n        except socket.timeout as e:\n            # socket timed out, try again\n            log.debug(e)\n\n        except Exception as e:\n            log.error(e, exc_info=True)\n            raise\n\n        break\n\n\ndef wait_for_operation(compute, project, operation):\n    print('Waiting for operation to finish...')\n    while True:\n        if 'zone' in operation:\n            operation = compute.zoneOperations().wait(\n                project=project,\n                zone=operation['zone'].split('/')[-1],\n                operation=operation['name'])\n        elif 'region' in operation:\n            operation = compute.regionOperations().wait(\n                project=project,\n                region=operation['region'].split('/')[-1],\n                operation=operation['name'])\n        else:\n            operation = compute.globalOperations().wait(\n                project=project,\n                operation=operation['name'])\n\n        result = ensure_execute(operation)\n        if result['status'] == 'DONE':\n            print(\"done.\")\n            return result\n\n\ndef get_group_operations(compute, project, operation):\n    \"\"\" get list of operations associated with group id \"\"\"\n\n    group_id = operation['operationGroupId']\n    if 'zone' in operation:\n        operation = compute.zoneOperations().list(\n            project=project,\n            zone=operation['zone'].split('/')[-1],\n            filter=f\"operationGroupId={group_id}\")\n    elif 'region' in operation:\n        operation = compute.regionOperations().list(\n            project=project,\n            region=operation['region'].split('/')[-1],\n            filter=f\"operationGroupId={group_id}\")\n    else:\n        operation = compute.globalOperations().list(\n            project=project,\n            filter=f\"operationGroupId={group_id}\")\n\n    return ensure_execute(operation)\n\n\ndef get_regional_instances(compute, project, def_list):\n    \"\"\" Get instances that exist in regional capacity instance defs \"\"\"\n\n    fields = 'items.zones.instances(name,zone,status),nextPageToken'\n    regional_instances = {}\n\n    region_filter = ' OR '.join(f'(name={pid}-*)' for pid, d in\n                                def_list.items() if d.regional_capacity)\n    if region_filter:\n        page_token = \"\"\n        while True:\n            resp = ensure_execute(\n                compute.instances().aggregatedList(\n                    project=project, filter=region_filter, fields=fields,\n                    pageToken=page_token))\n            if not resp:\n                break\n            for zone, zone_value in resp['items'].items():\n                if 'instances' in zone_value:\n                    regional_instances.update(\n                        {instance['name']: instance\n                         for instance in zone_value['instances']}\n                    )\n            if \"nextPageToken\" in resp:\n                page_token = resp['nextPageToken']\n                continue\n            break\n\n    return regional_instances\n"
            },
            "metadata_fingerprint": "lCaw1y0i5_A=",
            "metadata_startup_script": "#!/bin/bash\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nset -e\n\nFLAGFILE=/slurm/slurm_configured_do_not_remove\nif [ -f $FLAGFILE ]; then\n\techo \"Slurm was previously configured, quitting\"\n\texit 0\nfi\nmkdir -p $(dirname $FLAGFILE)\ntouch $FLAGFILE\n\nPING_HOST=8.8.8.8\nif ( ! ping -q -w1 -c1 $PING_HOST \u003e /dev/null ) ; then\n\techo No internet access detected\nfi\n\nSETUP_SCRIPT=\"setup.py\"\nSETUP_META=\"setup-script\"\nDIR=\"/tmp\"\nURL=\"http://metadata.google.internal/computeMetadata/v1/instance/attributes/$SETUP_META\"\nHEADER=\"Metadata-Flavor:Google\"\necho  \"wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT\"\nif ! ( wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT ) ; then\n    echo \"Failed to fetch $SETUP_META:$SETUP_SCRIPT from metadata\"\n    exit 1\nfi\n\necho \"running python cluster setup script\"\nchmod +x $DIR/$SETUP_SCRIPT\n$DIR/$SETUP_SCRIPT\n",
            "min_cpu_platform": "",
            "name": "wrf-demo-controller",
            "network_interface": [
              {
                "access_config": [],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
                "network_ip": "10.0.0.2",
                "nic_type": "",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/hpc-apps/regions/us-west1/subnetworks/wrf-demo-us-west1",
                "subnetwork_project": "hpc-apps"
              }
            ],
            "project": "hpc-apps",
            "resource_policies": null,
            "scheduling": [
              {
                "automatic_restart": true,
                "min_node_cpus": 0,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/zones/us-west1-b/instances/wrf-demo-controller",
            "service_account": [
              {
                "email": "909139830476-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "controller"
            ],
            "tags_fingerprint": "e2aTiSrlOyM=",
            "timeouts": null,
            "zone": "us-west1-b"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "module.slurm_cluster_controller.data.google_compute_default_service_account.default",
            "module.slurm_cluster_controller.google_compute_disk.secondary",
            "module.slurm_cluster_network.google_compute_network.cluster_network",
            "module.slurm_cluster_network.google_compute_subnetwork.cluster_subnet"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_login",
      "mode": "data",
      "type": "google_compute_default_service_account",
      "name": "default",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "display_name": "Compute Engine default service account",
            "email": "909139830476-compute@developer.gserviceaccount.com",
            "id": "projects/hpc-apps/serviceAccounts/909139830476-compute@developer.gserviceaccount.com",
            "name": "projects/hpc-apps/serviceAccounts/909139830476-compute@developer.gserviceaccount.com",
            "project": "hpc-apps",
            "unique_id": "113961341457955783623"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.slurm_cluster_login",
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "login_node",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": null,
            "attached_disk": [],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/images/wrf-gcp-v1",
                    "labels": {},
                    "size": 50,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/hpc-apps/zones/us-west1-b/disks/wrf-demo-login0"
              }
            ],
            "can_ip_forward": false,
            "confidential_instance_config": [],
            "cpu_platform": "Intel Broadwell",
            "current_status": "RUNNING",
            "deletion_protection": false,
            "description": "",
            "desired_status": null,
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/hpc-apps/zones/us-west1-b/instances/wrf-demo-login0",
            "instance_id": "1844119953473549071",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-16",
            "metadata": {
              "VmDnsSetting": "GlobalOnly",
              "config": "{\"cluster_name\":\"wrf-demo\",\"controller_secondary_disk\":false,\"login_network_storage\":[],\"munge_key\":null,\"network_storage\":[]}",
              "enable-oslogin": "TRUE",
              "setup-script": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport logging\nimport os\nimport sys\nimport shutil\nimport time\nfrom pathlib import Path\nfrom subprocess import DEVNULL\nfrom functools import reduce, partialmethod\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport googleapiclient.discovery\nimport requests\nimport yaml\n\n\n# get util.py from metadata\nUTIL_FILE = Path('/tmp/util.py')\ntry:\n    resp = requests.get('http://metadata.google.internal/computeMetadata/v1/instance/attributes/util-script',\n                        headers={'Metadata-Flavor': 'Google'})\n    resp.raise_for_status()\n    UTIL_FILE.write_text(resp.text)\nexcept requests.exceptions.RequestException:\n    print(\"util.py script not found in metadata\")\n    if not UTIL_FILE.exists():\n        print(f\"{UTIL_FILE} also does not exist, aborting\")\n        sys.exit(1)\n\nspec = importlib.util.spec_from_file_location('util', UTIL_FILE)\nutil = importlib.util.module_from_spec(spec)\nsys.modules[spec.name] = util\nspec.loader.exec_module(util)\ncd = util.cd  # import util.cd into local namespace\nNSDict = util.NSDict\n\nPath.mkdirp = partialmethod(Path.mkdir, parents=True, exist_ok=True)\n\nutil.config_root_logger(logfile='/tmp/setup.log')\nlog = logging.getLogger(Path(__file__).name)\nsys.excepthook = util.handle_exception\n\n# get setup config from metadata\nconfig_yaml = yaml.safe_load(util.get_metadata('attributes/config'))\ncfg = util.Config.new_config(config_yaml)\n\n# load all directories as Paths into a dict-like namespace\ndirs = NSDict({n: Path(p) for n, p in dict.items({\n    'home': '/home',\n    'apps': '/apps',\n    'scripts': '/slurm/scripts',\n    'slurm': '/slurm',\n    'prefix': '/usr/local',\n    'munge': '/etc/munge',\n    'secdisk': '/mnt/disks/sec',\n})})\n\nslurmdirs = NSDict({n: Path(p) for n, p in dict.items({\n    'etc': '/usr/local/etc/slurm',\n    'log': '/var/log/slurm',\n    'state': '/var/spool/slurm',\n})})\n\ncfg['log_dir'] = slurmdirs.log\ncfg['slurm_cmd_path'] = dirs.prefix/'bin'\n\nRESUME_TIMEOUT = 300\nSUSPEND_TIMEOUT = 300\n\nCONTROL_MACHINE = cfg.cluster_name + '-controller'\n\nMOTD_HEADER = \"\"\"\n\n                                 SSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                        SSSS     SSSSSSS     SSSS\n                       SSSSSS               SSSSSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                        SSSS    SSSSSSSSS    SSSS\n                SSS             SSSSSSSSS             SSS\n               SSSSS    SSSS    SSSSSSSSS    SSSS    SSSSS\n                SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                SSS    SSSSSS               SSSSSS    SSS\n               SSSSS    SSSS     SSSSSSS     SSSS    SSSSS\n          S     SSS             SSSSSSSSS             SSS     S\n         SSS            SSSS    SSSSSSSSS    SSSS            SSS\n          S     SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS     S\n               SSSSS   SSSSSS   SSSSSSSSS   SSSSSS   SSSSS\n          S    SSSSS    SSSS     SSSSSSS     SSSS    SSSSS    S\n    S    SSS    SSS                                   SSS    SSS    S\n    S     S                                                   S     S\n                SSS\n                SSS\n                SSS\n                SSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS    SSSSSSSSS   SSSSSSSSSSSSSSSSSSSS\nSSSSSSSSSSSSS   SSS   SSSS       SSSS   SSSSSSSSSS  SSSSSSSSSSSSSSSSSSSSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSSS   SSS   SSSSSSSSSSSSSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS    SSSSSSSSSSSSS    SSSS        SSSS     SSSS     SSSS\n\n\n\"\"\"\n\n\ndef start_motd():\n    \"\"\" advise in motd that slurm is currently configuring \"\"\"\n    msg = MOTD_HEADER + \"\"\"\n*** Slurm is currently being configured in the background. ***\n\"\"\"\n    Path('/etc/motd').write_text(msg)\n# END start_motd()\n\n\ndef end_motd(broadcast=True):\n    \"\"\" modify motd to signal that setup is complete \"\"\"\n    Path('/etc/motd').write_text(MOTD_HEADER)\n\n    if not broadcast:\n        return\n\n    util.run(\"wall -n '*** Slurm {} setup complete ***'\"\n             .format(cfg.instance_type))\n    if cfg.instance_type != 'controller':\n        util.run(\"\"\"wall -n '\n/home on the controller was mounted over the existing /home.\nLog back in to ensure your home directory is correct.\n'\"\"\")\n# END start_motd()\n\n\ndef expand_instance_templates():\n    \"\"\" Expand instance template into instance_defs \"\"\"\n\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for pid, instance_def in cfg.instance_defs.items():\n        if (instance_def.instance_template and\n                (not instance_def.machine_type or not instance_def.gpu_count)):\n            template_resp = util.ensure_execute(\n                compute.instanceTemplates().get(\n                    project=cfg.project,\n                    instanceTemplate=instance_def.instance_template))\n            if template_resp:\n                template_props = template_resp['properties']\n                if not instance_def.machine_type:\n                    instance_def.machine_type = template_props['machineType']\n                if (not instance_def.gpu_count and\n                        'guestAccelerators' in template_props):\n                    accel_props = template_props['guestAccelerators'][0]\n                    instance_def.gpu_count = accel_props['acceleratorCount']\n                    instance_def.gpu_type = accel_props['acceleratorType']\n# END expand_instance_templates()\n\n\ndef expand_machine_type():\n    \"\"\" get machine type specs from api \"\"\"\n    machines = {}\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for pid, part in cfg.instance_defs.items():\n        machine = {'cpus': 1, 'memory': 1}\n        machines[pid] = machine\n\n        if not part.machine_type:\n            log.error(\"No machine type to get configuration from\")\n            continue\n\n        type_resp = None\n        if part.regional_capacity:\n            filter = f\"(zone={part.region}-*) AND (name={part.machine_type})\"\n            list_resp = util.ensure_execute(\n                compute.machineTypes().aggregatedList(\n                    project=cfg.project, filter=filter))\n\n            if 'items' in list_resp:\n                zone_types = list_resp['items']\n                for k, v in zone_types.items():\n                    if part.region in k and 'machineTypes' in v:\n                        type_resp = v['machineTypes'][0]\n                        break\n        else:\n            type_resp = util.ensure_execute(\n                compute.machineTypes().get(\n                    project=cfg.project, zone=part.zone,\n                    machineType=part.machine_type))\n\n        if type_resp:\n            cpus = type_resp['guestCpus']\n            machine['cpus'] = cpus // (1 if part.image_hyperthreads else 2)\n\n            # Because the actual memory on the host will be different than\n            # what is configured (e.g. kernel will take it). From\n            # experiments, about 16 MB per GB are used (plus about 400 MB\n            # buffer for the first couple of GB's. Using 30 MB to be safe.\n            gb = type_resp['memoryMb'] // 1024\n            machine['memory'] = type_resp['memoryMb'] - (400 + (gb * 30))\n\n    return machines\n# END expand_machine_type()\n\n\ndef install_slurm_conf():\n    \"\"\" install slurm.conf \"\"\"\n    machines = expand_machine_type()\n\n    if cfg.ompi_version:\n        mpi_default = \"pmi2\"\n    else:\n        mpi_default = \"none\"\n\n    conf_options = {\n        'name': cfg.cluster_name,\n        'control_host': CONTROL_MACHINE,\n        'scripts': dirs.scripts,\n        'slurmlog': slurmdirs.log,\n        'state_save': slurmdirs.state,\n        'resume_timeout': RESUME_TIMEOUT,\n        'suspend_timeout': SUSPEND_TIMEOUT,\n        'suspend_time': cfg.suspend_time,\n        'mpi_default': mpi_default,\n    }\n    conf_resp = util.get_metadata('attributes/slurm_conf_tpl')\n    conf = conf_resp.format(**conf_options)\n\n    static_nodes = []\n    for i, (pid, machine) in enumerate(machines.items()):\n        part = cfg.instance_defs[pid]\n        static_range = ''\n        if part.static_node_count:\n            if part.static_node_count \u003e 1:\n                static_range = '{}-[0-{}]'.format(\n                    pid, part.static_node_count - 1)\n            else:\n                static_range = f\"{pid}-0\"\n\n        cloud_range = \"\"\n        if (part.max_node_count and\n                (part.max_node_count != part.static_node_count)):\n            cloud_range = \"{}-[{}-{}]\".format(\n                pid, part.static_node_count,\n                part.max_node_count - 1)\n\n        conf += (\"NodeName=DEFAULT \"\n                 f\"CPUs={machine['cpus']} \"\n                 f\"RealMemory={machine['memory']} \"\n                 \"State=UNKNOWN\")\n        conf += '\\n'\n\n        # Nodes\n        gres = \"\"\n        if part.gpu_count:\n            gres = \" Gres=gpu:\" + str(part.gpu_count)\n        if static_range:\n            static_nodes.append(static_range)\n            conf += f\"NodeName={static_range}{gres}\\n\"\n\n        if cloud_range:\n            conf += f\"NodeName={cloud_range} State=CLOUD{gres}\\n\"\n\n        # instance_defs\n        part_nodes = f'{pid}-[0-{part.max_node_count - 1}]'\n\n        def_mem_per_cpu = max(100, machine['memory'] // machine['cpus'])\n\n        conf += (\"PartitionName={} Nodes={} MaxTime=INFINITE \"\n                 \"State=UP DefMemPerCPU={} LLN=yes\"\n                 .format(part.name, part_nodes,\n                         def_mem_per_cpu))\n        if part.exclusive:\n            conf += \" Oversubscribe=Exclusive\"\n\n        # First partition specified is treated as the default partition\n        if i == 0:\n            conf += \" Default=YES\"\n        conf += \"\\n\\n\"\n\n    if len(static_nodes):\n        conf += \"\\nSuspendExcNodes={}\\n\".format(','.join(static_nodes))\n\n    conf_file = slurmdirs.etc/'slurm.conf'\n    conf_file.write_text(conf)\n    shutil.chown(conf_file, user='slurm', group='slurm')\n# END install_slurm_conf()\n\n\ndef install_slurmdbd_conf():\n    \"\"\" install slurmdbd.conf \"\"\"\n    conf_options = NSDict({\n        'control_host': CONTROL_MACHINE,\n        'slurmlog': slurmdirs.log,\n        'state_save': slurmdirs.state,\n        'db_name': 'slurm_acct_db',\n        'db_user': 'slurm',\n        'db_pass': '\"\"',\n        'db_host': 'localhost',\n        'db_port': '3306'\n    })\n    if cfg.cloudsql:\n        conf_options.db_name = cfg.cloudsql.db_name\n        conf_options.db_user = cfg.cloudsql.user\n        conf_options.db_pass = cfg.cloudsql.password\n\n        db_host_str = cfg.cloudsql.server_ip.split(':')\n        conf_options.db_host = db_host_str[0]\n        conf_options.db_port = db_host_str[1] if len(db_host_str) \u003e= 2 else '3306'\n\n    conf_resp = util.get_metadata('attributes/slurmdbd_conf_tpl')\n    conf = conf_resp.format(**conf_options)\n\n    conf_file = slurmdirs.etc/'slurmdbd.conf'\n    conf_file.write_text(conf)\n    shutil.chown(conf_file, user='slurm', group='slurm')\n    conf_file.chmod(0o600)\n# END install_slurmdbd_conf()\n\n\ndef install_cgroup_conf():\n    \"\"\" install cgroup.conf \"\"\"\n    conf = util.get_metadata('attributes/cgroup_conf_tpl')\n\n    conf_file = slurmdirs.etc/'cgroup.conf'\n    conf_file.write_text(conf)\n    shutil.chown(conf_file, user='slurm', group='slurm')\n\n    gpu_conf = \"\"\n    for pid, part in cfg.instance_defs.items():\n        if not part.gpu_count:\n            continue\n        driver_range = '0'\n        if part.gpu_count \u003e 1:\n            driver_range = '[0-{}]'.format(part.gpu_count-1)\n\n        gpu_conf += (\"NodeName={}-[0-{}] Name=gpu File=/dev/nvidia{}\\n\"\n                     .format(pid, part.max_node_count - 1, driver_range))\n    if gpu_conf:\n        (slurmdirs.etc/'gres.conf').write_text(gpu_conf)\n# END install_cgroup_conf()\n\n\ndef install_meta_files():\n    \"\"\" save config.yaml and download all scripts from metadata \"\"\"\n    cfg.save_config(dirs.scripts/'config.yaml')\n    shutil.chown(dirs.scripts/'config.yaml', user='slurm', group='slurm')\n\n    meta_entries = [\n        ('suspend.py', 'slurm-suspend'),\n        ('resume.py', 'slurm-resume'),\n        ('slurmsync.py', 'slurmsync'),\n        ('util.py', 'util-script'),\n        ('setup.py', 'setup-script'),\n        ('startup.sh', 'startup-script'),\n        ('custom-compute-install', 'custom-compute-install'),\n        ('custom-controller-install', 'custom-controller-install'),\n    ]\n\n    def install_metafile(filename, metaname):\n        text = util.get_metadata('attributes/' + metaname)\n        if not text:\n            return\n        path = dirs.scripts/filename\n        path.write_text(text)\n        path.chmod(0o755)\n        shutil.chown(path, user='slurm', group='slurm')\n\n    with ThreadPoolExecutor() as exe:\n        exe.map(lambda x: install_metafile(*x), meta_entries)\n\n# END install_meta_files()\n\n\ndef prepare_network_mounts(hostname, instance_type):\n    \"\"\" Prepare separate lists of cluster-internal and external mounts for the\n    given host instance, returning (external_mounts, internal_mounts)\n    \"\"\"\n    log.info(\"Set up network storage\")\n\n    default_mounts = (\n        slurmdirs.etc,\n        dirs.munge,\n        dirs.home,\n        dirs.apps,\n    )\n\n    # create dict of mounts, local_mount: mount_info\n    CONTROL_NFS = {\n        'server_ip': CONTROL_MACHINE,\n        'remote_mount': 'none',\n        'local_mount': 'none',\n        'fs_type': 'nfs',\n        'mount_options': 'defaults,hard,intr',\n    }\n    # seed the non-controller mounts with the default controller mounts\n    mounts = {\n        path: util.Config(CONTROL_NFS, local_mount=path, remote_mount=path)\n        for path in default_mounts\n    }\n\n    # convert network_storage list of mounts to dict of mounts,\n    #   local_mount as key\n    def listtodict(mountlist):\n        return {Path(d['local_mount']).resolve(): d for d in mountlist}\n\n    # On non-controller instances, entries in network_storage could overwrite\n    # default exports from the controller. Be careful, of course\n    mounts.update(listtodict(cfg.network_storage))\n\n    if instance_type == 'compute':\n        pid = util.get_pid(hostname)\n        mounts.update(listtodict(cfg.instance_defs[pid].network_storage))\n    else:\n        # login_network_storage is mounted on controller and login instances\n        mounts.update(listtodict(cfg.login_network_storage))\n\n    # filter mounts into two dicts, cluster-internal and external mounts, and\n    # return both. (external_mounts, internal_mounts)\n    def internal_mount(mount):\n        return mount[1].server_ip == CONTROL_MACHINE\n\n    def partition(pred, coll):\n        \"\"\" filter into 2 lists based on pred returning True or False \n            returns ([False], [True])\n        \"\"\"\n        return reduce(\n            lambda acc, el: acc[pred(el)].append(el) or acc,\n            coll, ([], [])\n        )\n\n    return tuple(map(dict, partition(internal_mount, mounts.items())))\n# END prepare_network_mounts\n\n\ndef setup_network_storage():\n    \"\"\" prepare network fs mounts and add them to fstab \"\"\"\n\n    global mounts\n    ext_mounts, int_mounts = prepare_network_mounts(cfg.hostname,\n                                                    cfg.instance_type)\n    mounts = ext_mounts\n    if cfg.instance_type != 'controller':\n        mounts.update(int_mounts)\n\n    # Determine fstab entries and write them out\n    fstab_entries = []\n    for local_mount, mount in mounts.items():\n        remote_mount = mount.remote_mount\n        fs_type = mount.fs_type\n        server_ip = mount.server_ip\n\n        # do not mount controller mounts to itself\n        if server_ip == CONTROL_MACHINE and cfg.instance_type == 'controller':\n            continue\n\n        log.info(\"Setting up mount ({}) {}{} to {}\".format(\n            fs_type, server_ip+':' if fs_type != 'gcsfuse' else \"\",\n            remote_mount, local_mount))\n\n        local_mount.mkdirp()\n\n        mount_options = (mount.mount_options.split(',') if mount.mount_options\n                         else [])\n        if not mount_options or '_netdev' not in mount_options:\n            mount_options += ['_netdev']\n\n        if fs_type == 'gcsfuse':\n            if 'nonempty' not in mount_options:\n                mount_options += ['nonempty']\n            fstab_entries.append(\n                \"{0}   {1}     {2}     {3}     0 0\"\n                .format(remote_mount, local_mount, fs_type,\n                        ','.join(mount_options)))\n        else:\n            remote_mount = Path(remote_mount).resolve()\n            fstab_entries.append(\n                \"{0}:{1}    {2}     {3}      {4}  0 0\"\n                .format(server_ip, remote_mount, local_mount,\n                        fs_type, ','.join(mount_options)))\n\n    for mount in mounts:\n        Path(mount).mkdirp()\n    with open('/etc/fstab', 'a') as f:\n        f.write('\\n')\n        for entry in fstab_entries:\n            f.write(entry)\n            f.write('\\n')\n# END setup_network_storage()\n\n\ndef mount_fstab():\n    \"\"\" Wait on each mount, then make sure all fstab is mounted \"\"\"\n    global mounts\n\n    def mount_path(path):\n        while not os.path.ismount(path):\n            log.info(f\"Waiting for {path} to be mounted\")\n            util.run(f\"mount {path}\", wait=5)\n\n    with ThreadPoolExecutor() as exe:\n        exe.map(mount_path, mounts.keys())\n\n    util.run(\"mount -a\", wait=1)\n# END mount_external\n\n\ndef setup_nfs_exports():\n    \"\"\" nfs export all needed directories \"\"\"\n    # The controller only needs to set up exports for cluster-internal mounts\n    # switch the key to remote mount path since that is what needs exporting\n    _, con_mounts = prepare_network_mounts(cfg.hostname, cfg.instance_type)\n    con_mounts = {m.remote_mount: m for m in con_mounts.values()}\n    for pid, _ in cfg.instance_defs.items():\n        # get internal mounts for each partition by calling\n        # prepare_network_mounts as from a node in each partition\n        _, part_mounts = prepare_network_mounts(f'{pid}-n', 'compute')\n        part_mounts = {m.remote_mount: m for m in part_mounts.values()}\n        con_mounts.update(part_mounts)\n\n    # export path if corresponding selector boolean is True\n    exports = []\n    for path in con_mounts:\n        Path(path).mkdirp()\n        util.run(rf\"sed -i '\\#{path}#d' /etc/exports\")\n        exports.append(f\"{path}  *(rw,no_subtree_check,no_root_squash)\")\n\n    exportsd = Path('/etc/exports.d')\n    exportsd.mkdirp()\n    with (exportsd/'slurm.exports').open('w') as f:\n        f.write('\\n')\n        f.write('\\n'.join(exports))\n    util.run(\"exportfs -a\")\n# END setup_nfs_exports()\n\n\ndef setup_secondary_disks():\n    \"\"\" Format and mount secondary disk \"\"\"\n    util.run(\n        \"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\")\n    with open('/etc/fstab', 'a') as f:\n        f.write(\n            \"\\n/dev/sdb     {0}     ext4    discard,defaults,nofail     0 2\"\n            .format(dirs.secdisk))\n\n# END setup_secondary_disks()\n\n\ndef setup_sync_cronjob():\n    \"\"\" Create cronjob for running slurmsync.py \"\"\"\n    util.run(\"crontab -u slurm -\", input=(\n        f\"*/1 * * * * {dirs.scripts}/slurmsync.py\\n\"))\n\n# END setup_sync_cronjob()\n\n\ndef setup_jwt_key():\n    jwt_key = slurmdirs.state/'jwt_hs256.key'\n\n    if cfg.jwt_key:\n        with (jwt_key).open('w') as f:\n            f.write(cfg.jwt_key)\n    else:\n        util.run(\"dd if=/dev/urandom bs=32 count=1 \u003e\"+str(jwt_key), shell=True)\n\n    util.run(f\"chown -R slurm:slurm {jwt_key}\")\n    jwt_key.chmod(0o400)\n\n\ndef setup_slurmd_cronjob():\n    \"\"\" Create cronjob for keeping slurmd service up \"\"\"\n    util.run(\n        \"crontab -u root -\", input=(\n            \"*/2 * * * * \"\n            \"if [ `systemctl status slurmd | grep -c inactive` -gt 0 ]; then \"\n            \"mount -a; \"\n            \"systemctl restart munge; \"\n            \"systemctl restart slurmd; \"\n            \"fi\\n\"\n        ))\n# END setup_slurmd_cronjob()\n\n\ndef setup_nss_slurm():\n    \"\"\" install and configure nss_slurm \"\"\"\n    # setup nss_slurm\n    Path('/var/spool/slurmd').mkdirp()\n    util.run(\"ln -s {}/lib/libnss_slurm.so.2 /usr/lib64/libnss_slurm.so.2\"\n             .format(dirs.prefix))\n    util.run(\n        r\"sed -i 's/\\(^\\(passwd\\|group\\):\\s\\+\\)/\\1slurm /g' /etc/nsswitch.conf\"\n    )\n# END setup_nss_slurm()\n\n\ndef configure_dirs():\n\n    for p in dirs.values():\n        p.mkdirp()\n    shutil.chown(dirs.slurm, user='slurm', group='slurm')\n    shutil.chown(dirs.scripts, user='slurm', group='slurm')\n\n    for p in slurmdirs.values():\n        p.mkdirp()\n        shutil.chown(p, user='slurm', group='slurm')\n\n    (dirs.scripts/'etc').symlink_to(slurmdirs.etc)\n    shutil.chown(dirs.scripts/'etc', user='slurm', group='slurm')\n\n    (dirs.scripts/'log').symlink_to(slurmdirs.log)\n    shutil.chown(dirs.scripts/'log', user='slurm', group='slurm')\n\n\ndef setup_controller():\n    \"\"\" Run controller setup \"\"\"\n    expand_instance_templates()\n    install_cgroup_conf()\n    install_slurm_conf()\n    install_slurmdbd_conf()\n    setup_jwt_key()\n    util.run(\"create-munge-key -f\")\n    util.run(\"systemctl restart munge\")\n\n    if cfg.controller_secondary_disk:\n        setup_secondary_disks()\n    setup_network_storage()\n    mount_fstab()\n\n    try:\n        util.run(str(dirs.scripts/'custom-controller-install'))\n    except Exception:\n        # Ignore blank files with no shell magic.\n        pass\n\n    if not cfg.cloudsql:\n        cnfdir = Path('/etc/my.cnf.d')\n        if not cnfdir.exists():\n            cnfdir = Path('/etc/mysql/conf.d')\n        (cnfdir/'mysql_slurm.cnf').write_text(\"\"\"\n[mysqld]\nbind-address = 127.0.0.1\n\"\"\")\n        util.run('systemctl enable mariadb')\n        util.run('systemctl start mariadb')\n\n        mysql = \"mysql -u root -e\"\n        util.run(\n            f\"\"\"{mysql} \"create user 'slurm'@'localhost'\";\"\"\")\n        util.run(\n            f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'localhost'\";\"\"\")\n        util.run(\n            f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'{CONTROL_MACHINE}'\";\"\"\")\n\n    util.run(\"systemctl enable slurmdbd\")\n    util.run(\"systemctl start slurmdbd\")\n\n    # Wait for slurmdbd to come up\n    time.sleep(5)\n\n    sacctmgr = f\"{dirs.prefix}/bin/sacctmgr -i\"\n    util.run(f\"{sacctmgr} add cluster {cfg.cluster_name}\")\n\n    util.run(\"systemctl enable slurmctld\")\n    util.run(\"systemctl start slurmctld\")\n\n    util.run(\"systemctl enable slurmrestd\")\n    util.run(\"systemctl start slurmrestd\")\n\n    # Export at the end to signal that everything is up\n    util.run(\"systemctl enable nfs-server\")\n    util.run(\"systemctl start nfs-server\")\n\n    setup_nfs_exports()\n    setup_sync_cronjob()\n\n    log.info(\"Done setting up controller\")\n    pass\n\n\ndef setup_login():\n    \"\"\" run login node setup \"\"\"\n    setup_network_storage()\n    mount_fstab()\n    util.run(\"systemctl restart munge\")\n\n    try:\n        util.run(str(dirs.scripts/'custom-compute-install'))\n    except Exception:\n        # Ignore blank files with no shell magic.\n        pass\n    log.info(\"Done setting up login\")\n\n\ndef setup_compute():\n    \"\"\" run compute node setup \"\"\"\n    setup_nss_slurm()\n    setup_network_storage()\n    mount_fstab()\n\n    pid = util.get_pid(cfg.hostname)\n    if cfg.instance_defs[pid].gpu_count:\n        retries = n = 50\n        while util.run(\"nvidia-smi\").returncode != 0 and n \u003e 0:\n            n -= 1\n            log.info(f\"Nvidia driver not yet loaded, try {retries-n}\")\n            time.sleep(5)\n\n    try:\n        util.run(str(dirs.scripts/'custom-compute-install'))\n    except Exception:\n        # Ignore blank files with no shell magic.\n        pass\n\n    setup_slurmd_cronjob()\n    util.run(\"systemctl restart munge\")\n    util.run(\"systemctl enable slurmd\")\n    util.run(\"systemctl start slurmd\")\n\n    log.info(\"Done setting up compute\")\n\n\ndef main():\n\n    start_motd()\n    configure_dirs()\n    install_meta_files()\n\n    # call the setup function for the instance type\n    setup = dict.get(\n        {\n            'controller': setup_controller,\n            'compute': setup_compute,\n            'login': setup_login\n        },\n        cfg.instance_type,\n        lambda: log.fatal(f\"Unknown instance type: {cfg.instance_type}\")\n    )\n    setup()\n\n    end_motd()\n# END main()\n\n\nif __name__ == '__main__':\n    main()\n",
              "util-script": "#!/usr/bin/env python3\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport logging.config\nimport os\nimport shlex\nimport subprocess\nimport sys\nimport socket\nimport time\nfrom itertools import chain, compress\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\nimport googleapiclient.discovery\n\nimport requests\nimport yaml\n\n\nlog = logging.getLogger(__name__)\n\n\ndef config_root_logger(level='DEBUG', util_level=None,\n                       stdout=True, logfile=None):\n    if not util_level:\n        util_level = level\n    handlers = list(compress(('stdout_handler', 'file_handler'),\n                             (stdout, logfile)))\n\n    config = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'standard': {\n                'format': '',\n            },\n            'stamp': {\n                'format': '%(asctime)s %(process)s %(thread)s %(name)s %(levelname)s: %(message)s',\n            },\n        },\n        'handlers': {\n            'stdout_handler': {\n                'level': 'DEBUG',\n                'formatter': 'standard',\n                'class': 'logging.StreamHandler',\n                'stream': sys.stdout,\n            },\n        },\n        'loggers': {\n            __name__: {  # enable util.py logging\n                'level': util_level,\n            },\n        },\n        'root': {\n            'handlers': handlers,\n            'level': level,\n        }\n    }\n    if logfile:\n        config['handlers']['file_handler'] = {\n            'level': 'DEBUG',\n            'formatter': 'stamp',\n            'class': 'logging.handlers.WatchedFileHandler',\n            'filename': logfile,\n        }\n    logging.config.dictConfig(config)\n\n\ndef handle_exception(exc_type, exc_value, exc_trace):\n    if not issubclass(exc_type, KeyboardInterrupt):\n        log.exception(\"Fatal exception\",\n                      exc_info=(exc_type, exc_value, exc_trace))\n    sys.__excepthook__(exc_type, exc_value, exc_trace)\n\n\ndef get_metadata(path):\n    \"\"\" Get metadata relative to metadata/computeMetadata/v1/instance/ \"\"\"\n    URL = 'http://metadata.google.internal/computeMetadata/v1/instance/'\n    HEADERS = {'Metadata-Flavor': 'Google'}\n    full_path = URL + path\n    try:\n        resp = requests.get(full_path, headers=HEADERS)\n        resp.raise_for_status()\n    except requests.exceptions.RequestException:\n        log.error(f\"Error while getting metadata from {full_path}\")\n        return None\n    return resp.text\n\n\ndef run(cmd, wait=0, quiet=False, get_stdout=False,\n        shell=False, universal_newlines=True, **kwargs):\n    \"\"\" run in subprocess. Optional wait after return. \"\"\"\n    if not quiet:\n        log.debug(f\"run: {cmd}\")\n    if get_stdout:\n        kwargs['stdout'] = subprocess.PIPE\n\n    args = cmd if shell else shlex.split(cmd)\n    ret = subprocess.run(args, shell=shell,\n                         universal_newlines=universal_newlines,\n                         **kwargs)\n    if wait:\n        time.sleep(wait)\n    return ret\n\n\ndef spawn(cmd, quiet=False, shell=False, **kwargs):\n    \"\"\" nonblocking spawn of subprocess \"\"\"\n    if not quiet:\n        log.debug(f\"spawn: {cmd}\")\n    args = cmd if shell else shlex.split(cmd)\n    return subprocess.Popen(args, shell=shell, **kwargs)\n\n\ndef get_pid(node_name):\n    \"\"\"Convert \u003cprefix\u003e-\u003cpid\u003e-\u003cnid\u003e\"\"\"\n\n    return '-'.join(node_name.split('-')[:-1])\n\n\n@contextmanager\ndef cd(path):\n    \"\"\" Change working directory for context \"\"\"\n    prev = Path.cwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(prev)\n\n\ndef static_vars(**kwargs):\n    \"\"\"\n    Add variables to the function namespace.\n    @static_vars(var=init): var must be referenced func.var\n    \"\"\"\n    def decorate(func):\n        for k in kwargs:\n            setattr(func, k, kwargs[k])\n        return func\n    return decorate\n\n\nclass cached_property:\n    \"\"\"\n    Descriptor for creating a property that is computed once and cached\n    \"\"\"\n    def __init__(self, factory):\n        self._attr_name = factory.__name__\n        self._factory = factory\n\n    def __get__(self, instance, owner=None):\n        if instance is None:  # only if invoked from class\n            return self\n        attr = self._factory(instance)\n        setattr(instance, self._attr_name, attr)\n        return attr\n\n\nclass NSDict(OrderedDict):\n    \"\"\" Simple nested dict namespace \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        def from_nested(value):\n            \"\"\" If value is dict, convert to NSDict. Also recurse lists. \"\"\"\n            if isinstance(value, dict):\n                return type(self)({k: from_nested(v) for k, v in value.items()})\n            elif isinstance(value, list):\n                return [from_nested(v) for v in value]\n            else:\n                return value\n\n        super(NSDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self  # all properties are member attributes\n\n        # Convert nested elements\n        for k, v in self.items():\n            self[k] = from_nested(v)\n\n\nclass Config(NSDict):\n    \"\"\" Loads config from yaml and holds values in nested namespaces \"\"\"\n\n    TYPES = set(('compute', 'login', 'controller'))\n    # PROPERTIES defines which properties in slurm.jinja.schema are included\n    #   in the config file. SAVED_PROPS are saved to file via save_config.\n    SAVED_PROPS = ('project',\n                   'zone',\n                   'cluster_name',\n                   'external_compute_ips',\n                   'shared_vpc_host_project',\n                   'compute_node_service_account',\n                   'compute_node_scopes',\n                   'slurm_cmd_path',\n                   'log_dir',\n                   'google_app_cred_path',\n                   'update_node_addrs',\n                   'network_storage',\n                   'login_network_storage',\n                   'instance_defs',\n                   )\n    PROPERTIES = (*SAVED_PROPS,\n                  'munge_key',\n                  'jwt_key',\n                  'external_compute_ips',\n                  'controller_secondary_disk',\n                  'suspend_time',\n                  'login_node_count',\n                  'cloudsql',\n                  'partitions',\n                  )\n\n    def __init__(self, *args, **kwargs):\n        super(Config, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def new_config(cls, properties):\n        # If k is ever not found, None will be inserted as the value\n        cfg = cls({k: properties.setdefault(k, None) for k in cls.PROPERTIES})\n        if cfg.partitions:\n            cfg['instance_defs'] = Config({\n                f'{cfg.cluster_name}-compute-{pid}': part\n                for pid, part in enumerate(cfg.partitions)\n            })\n\n        for netstore in (*cfg.network_storage, *(cfg.login_network_storage or []),\n                         *chain(*(p.network_storage for p in (cfg.partitions or [])))):\n            if netstore.server_ip == '$controller':\n                netstore.server_ip = cfg.cluster_name + '-controller'\n        return cfg\n\n    @classmethod\n    def load_config(cls, path):\n        config = yaml.safe_load(Path(path).read_text())\n        return cls(config)\n\n    def save_config(self, path):\n        save_dict = Config([(k, self[k]) for k in self.SAVED_PROPS])\n        if save_dict.instance_defs:\n            for instance_type in save_dict.instance_defs.values():\n                instance_type.pop('max_node_count', 0)\n                instance_type.pop('name', 0)\n                instance_type.pop('static_node_count', 0)\n        Path(path).write_text(yaml.dump(save_dict, Dumper=Dumper))\n\n    @cached_property\n    def instance_type(self):\n        # get tags, intersect with possible types, get the first or none\n        tags = yaml.safe_load(get_metadata('tags'))\n        # TODO what to default to if no match found.\n        return next(iter(set(tags) \u0026 self.TYPES), None)\n\n    @cached_property\n    def hostname(self):\n        return socket.gethostname()\n\n    @property\n    def region(self):\n        if self.zone:\n            parts = self.zone.split('-')\n            if len(parts) \u003e 2:\n                return '-'.join(parts[:-1])\n            else:\n                return self.zone\n        return None\n\n    @property\n    def exclusive(self):\n        return bool(self.get('exclusive', False) or self.enable_placement)\n\n    def __getattr__(self, item):\n        \"\"\" only called if item is not found in self \"\"\"\n        return None\n\n\nclass Dumper(yaml.SafeDumper):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.add_representer(Config, self.represent_nsdict)\n        self.add_representer(NSDict, self.represent_nsdict)\n        self.add_multi_representer(Path, self.represent_path)\n\n    @staticmethod\n    def represent_nsdict(dumper, data):\n        return dumper.represent_mapping('tag:yaml.org,2002:map',\n                                        data.items())\n\n    @staticmethod\n    def represent_path(dumper, path):\n        return dumper.represent_scalar('tag:yaml.org,2002:str',\n                                       str(path))\n\n\ndef ensure_execute(operation):\n    \"\"\" Handle rate limits and socket time outs \"\"\"\n\n    retry = 0\n    sleep = 1\n    max_sleep = 60\n    while True:\n        try:\n            return operation.execute()\n\n        except googleapiclient.errors.HttpError as e:\n            if \"Rate Limit Exceeded\" in str(e):\n                retry += 1\n                sleep = min(sleep*2, max_sleep)\n                log.error(f\"retry:{retry} sleep:{sleep} '{e}'\")\n                time.sleep(sleep)\n                continue\n            raise\n\n        except socket.timeout as e:\n            # socket timed out, try again\n            log.debug(e)\n\n        except Exception as e:\n            log.error(e, exc_info=True)\n            raise\n\n        break\n\n\ndef wait_for_operation(compute, project, operation):\n    print('Waiting for operation to finish...')\n    while True:\n        if 'zone' in operation:\n            operation = compute.zoneOperations().wait(\n                project=project,\n                zone=operation['zone'].split('/')[-1],\n                operation=operation['name'])\n        elif 'region' in operation:\n            operation = compute.regionOperations().wait(\n                project=project,\n                region=operation['region'].split('/')[-1],\n                operation=operation['name'])\n        else:\n            operation = compute.globalOperations().wait(\n                project=project,\n                operation=operation['name'])\n\n        result = ensure_execute(operation)\n        if result['status'] == 'DONE':\n            print(\"done.\")\n            return result\n\n\ndef get_group_operations(compute, project, operation):\n    \"\"\" get list of operations associated with group id \"\"\"\n\n    group_id = operation['operationGroupId']\n    if 'zone' in operation:\n        operation = compute.zoneOperations().list(\n            project=project,\n            zone=operation['zone'].split('/')[-1],\n            filter=f\"operationGroupId={group_id}\")\n    elif 'region' in operation:\n        operation = compute.regionOperations().list(\n            project=project,\n            region=operation['region'].split('/')[-1],\n            filter=f\"operationGroupId={group_id}\")\n    else:\n        operation = compute.globalOperations().list(\n            project=project,\n            filter=f\"operationGroupId={group_id}\")\n\n    return ensure_execute(operation)\n\n\ndef get_regional_instances(compute, project, def_list):\n    \"\"\" Get instances that exist in regional capacity instance defs \"\"\"\n\n    fields = 'items.zones.instances(name,zone,status),nextPageToken'\n    regional_instances = {}\n\n    region_filter = ' OR '.join(f'(name={pid}-*)' for pid, d in\n                                def_list.items() if d.regional_capacity)\n    if region_filter:\n        page_token = \"\"\n        while True:\n            resp = ensure_execute(\n                compute.instances().aggregatedList(\n                    project=project, filter=region_filter, fields=fields,\n                    pageToken=page_token))\n            if not resp:\n                break\n            for zone, zone_value in resp['items'].items():\n                if 'instances' in zone_value:\n                    regional_instances.update(\n                        {instance['name']: instance\n                         for instance in zone_value['instances']}\n                    )\n            if \"nextPageToken\" in resp:\n                page_token = resp['nextPageToken']\n                continue\n            break\n\n    return regional_instances\n"
            },
            "metadata_fingerprint": "i2OGCfeFibg=",
            "metadata_startup_script": "#!/bin/bash\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nset -e\n\nFLAGFILE=/slurm/slurm_configured_do_not_remove\nif [ -f $FLAGFILE ]; then\n\techo \"Slurm was previously configured, quitting\"\n\texit 0\nfi\nmkdir -p $(dirname $FLAGFILE)\ntouch $FLAGFILE\n\nPING_HOST=8.8.8.8\nif ( ! ping -q -w1 -c1 $PING_HOST \u003e /dev/null ) ; then\n\techo No internet access detected\nfi\n\nSETUP_SCRIPT=\"setup.py\"\nSETUP_META=\"setup-script\"\nDIR=\"/tmp\"\nURL=\"http://metadata.google.internal/computeMetadata/v1/instance/attributes/$SETUP_META\"\nHEADER=\"Metadata-Flavor:Google\"\necho  \"wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT\"\nif ! ( wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT ) ; then\n    echo \"Failed to fetch $SETUP_META:$SETUP_SCRIPT from metadata\"\n    exit 1\nfi\n\necho \"running python cluster setup script\"\nchmod +x $DIR/$SETUP_SCRIPT\n$DIR/$SETUP_SCRIPT\n",
            "min_cpu_platform": "",
            "name": "wrf-demo-login0",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "35.197.20.178",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
                "network_ip": "10.0.0.3",
                "nic_type": "",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/hpc-apps/regions/us-west1/subnetworks/wrf-demo-us-west1",
                "subnetwork_project": "hpc-apps"
              }
            ],
            "project": "hpc-apps",
            "resource_policies": null,
            "scheduling": [
              {
                "automatic_restart": true,
                "min_node_cpus": 0,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/zones/us-west1-b/instances/wrf-demo-login0",
            "service_account": [
              {
                "email": "909139830476-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/logging.write",
                  "https://www.googleapis.com/auth/monitoring.write"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "login"
            ],
            "tags_fingerprint": "2KMAI-0fjjw=",
            "timeouts": null,
            "zone": "us-west1-b"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "module.slurm_cluster_login.data.google_compute_default_service_account.default",
            "module.slurm_cluster_network.google_compute_network.cluster_network",
            "module.slurm_cluster_network.google_compute_subnetwork.cluster_subnet"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_firewall",
      "name": "cluster_internal_firewall",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "allow": [
              {
                "ports": [
                  "0-65535"
                ],
                "protocol": "tcp"
              },
              {
                "ports": [
                  "0-65535"
                ],
                "protocol": "udp"
              },
              {
                "ports": [],
                "protocol": "icmp"
              }
            ],
            "creation_timestamp": "2021-05-25T16:52:40.459-07:00",
            "deny": [],
            "description": "",
            "destination_ranges": [],
            "direction": "INGRESS",
            "disabled": false,
            "enable_logging": null,
            "id": "projects/hpc-apps/global/firewalls/wrf-demo-allow-internal",
            "log_config": [],
            "name": "wrf-demo-allow-internal",
            "network": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
            "priority": 1000,
            "project": "hpc-apps",
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/firewalls/wrf-demo-allow-internal",
            "source_ranges": [
              "10.0.0.0/8"
            ],
            "source_service_accounts": null,
            "source_tags": null,
            "target_service_accounts": null,
            "target_tags": null,
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoyNDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_network.cluster_network"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_firewall",
      "name": "cluster_ssh_firewall",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "allow": [
              {
                "ports": [
                  "22"
                ],
                "protocol": "tcp"
              }
            ],
            "creation_timestamp": "2021-05-25T16:52:40.229-07:00",
            "deny": [],
            "description": "",
            "destination_ranges": [],
            "direction": "INGRESS",
            "disabled": false,
            "enable_logging": null,
            "id": "projects/hpc-apps/global/firewalls/wrf-demo-allow-ssh",
            "log_config": [],
            "name": "wrf-demo-allow-ssh",
            "network": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
            "priority": 1000,
            "project": "hpc-apps",
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/firewalls/wrf-demo-allow-ssh",
            "source_ranges": [
              "0.0.0.0/0"
            ],
            "source_service_accounts": null,
            "source_tags": null,
            "target_service_accounts": null,
            "target_tags": null,
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoyNDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_network.cluster_network"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_network",
      "name": "cluster_network",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "auto_create_subnetworks": false,
            "delete_default_routes_on_create": false,
            "description": "",
            "gateway_ipv4": "",
            "id": "projects/hpc-apps/global/networks/wrf-demo-network",
            "mtu": 0,
            "name": "wrf-demo-network",
            "project": "hpc-apps",
            "routing_mode": "REGIONAL",
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoyNDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19"
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_router",
      "name": "cluster_router",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "bgp": [],
            "creation_timestamp": "2021-05-25T16:52:42.634-07:00",
            "description": "",
            "id": "projects/hpc-apps/regions/us-west1/routers/wrf-demo-router",
            "name": "wrf-demo-router",
            "network": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
            "project": "hpc-apps",
            "region": "us-west1",
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/regions/us-west1/routers/wrf-demo-router",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoyNDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_network.cluster_network"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_router_nat",
      "name": "cluster_nat",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "drain_nat_ips": null,
            "enable_endpoint_independent_mapping": true,
            "icmp_idle_timeout_sec": 30,
            "id": "hpc-apps/us-west1/wrf-demo-router/wrf-demo-nat",
            "log_config": [
              {
                "enable": true,
                "filter": "ERRORS_ONLY"
              }
            ],
            "min_ports_per_vm": 0,
            "name": "wrf-demo-nat",
            "nat_ip_allocate_option": "AUTO_ONLY",
            "nat_ips": null,
            "project": "hpc-apps",
            "region": "us-west1",
            "router": "wrf-demo-router",
            "source_subnetwork_ip_ranges_to_nat": "LIST_OF_SUBNETWORKS",
            "subnetwork": [
              {
                "name": "wrf-demo-us-west1",
                "secondary_ip_range_names": [],
                "source_ip_ranges_to_nat": [
                  "PRIMARY_IP_RANGE"
                ]
              }
            ],
            "tcp_established_idle_timeout_sec": 1200,
            "tcp_transitory_idle_timeout_sec": 30,
            "timeouts": null,
            "udp_idle_timeout_sec": 30
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6NjAwMDAwMDAwMDAwLCJ1cGRhdGUiOjYwMDAwMDAwMDAwMH19",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_network.cluster_network",
            "module.slurm_cluster_network.google_compute_router.cluster_router",
            "module.slurm_cluster_network.google_compute_subnetwork.cluster_subnet"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_subnetwork",
      "name": "cluster_subnet",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "creation_timestamp": "2021-05-25T16:52:42.999-07:00",
            "description": "",
            "fingerprint": null,
            "gateway_address": "10.0.0.1",
            "id": "projects/hpc-apps/regions/us-west1/subnetworks/wrf-demo-us-west1",
            "ip_cidr_range": "10.0.0.0/16",
            "log_config": [],
            "name": "wrf-demo-us-west1",
            "network": "https://www.googleapis.com/compute/v1/projects/hpc-apps/global/networks/wrf-demo-network",
            "private_ip_google_access": true,
            "private_ipv6_google_access": "DISABLE_GOOGLE_ACCESS",
            "project": "hpc-apps",
            "region": "us-west1",
            "secondary_ip_range": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/hpc-apps/regions/us-west1/subnetworks/wrf-demo-us-west1",
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozNjAwMDAwMDAwMDAsImRlbGV0ZSI6MzYwMDAwMDAwMDAwLCJ1cGRhdGUiOjM2MDAwMDAwMDAwMH19",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_network.cluster_network"
          ]
        }
      ]
    }
  ]
}
